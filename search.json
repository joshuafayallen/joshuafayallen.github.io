[
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "Introduction to American Government \n                \n            \n            \n                Pols 1101 | \n                Georgia State University\n                \n            \n            Introduction to American Government\n\n            \n                \n                \n                 Fall 2022\n                \n                \n                \n                 Summer 2022\n                \n                \n                \n                 Spring 2022\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Introduction to Political Science Research \n                \n            \n            \n                Pols 3800 | \n                Georgia State University\n                \n            \n            This course serves as an introduction research methods, casual inference, applied statistics, and using R. During the semester we will cover how to develop research questions, theory development, and how to answer questions about political phenomena\n\n            \n                \n                 \n                 Spring 2023\n                \n                \n                 \n                 Summer 2023\n                \n                \n                \n                 Fall 2023\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                R Workshops \n                \n            \n            \n                 | \n                Georgia State University\n                \n            \n            Research Data Services R Workshops\n\n            \n                \n                \n                 Spring 2023\n                \n                \n                 \n                 Fall 2022\n                \n                \n            \n        \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#section",
    "href": "teaching/index.html#section",
    "title": "Teaching",
    "section": "",
    "text": "Introduction to American Government \n                \n            \n            \n                Pols 1101 | \n                Georgia State University\n                \n            \n            Introduction to American Government\n\n            \n                \n                \n                 Fall 2022\n                \n                \n                \n                 Summer 2022\n                \n                \n                \n                 Spring 2022\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Introduction to Political Science Research \n                \n            \n            \n                Pols 3800 | \n                Georgia State University\n                \n            \n            This course serves as an introduction research methods, casual inference, applied statistics, and using R. During the semester we will cover how to develop research questions, theory development, and how to answer questions about political phenomena\n\n            \n                \n                 \n                 Spring 2023\n                \n                \n                 \n                 Summer 2023\n                \n                \n                \n                 Fall 2023\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                R Workshops \n                \n            \n            \n                 | \n                Georgia State University\n                \n            \n            Research Data Services R Workshops\n\n            \n                \n                \n                 Spring 2023\n                \n                \n                 \n                 Fall 2022\n                \n                \n            \n        \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Josh Allen",
    "section": "",
    "text": "Welcome to my website! My name is Josh Allen and I earned my PhD in the Department of Political Science at Georgia State University. I earned my M.A. at Georgia State and am a proud Sonoma State University alum.\nMy research focused on the impact of the Holocaust on contemporary political attitudes in Europe using causal inference tools and Large Language Models."
  },
  {
    "objectID": "blog/2025/passing-td-factor-model/index.html",
    "href": "blog/2025/passing-td-factor-model/index.html",
    "title": "Modeling Receiver Ability Bayesiansly",
    "section": "",
    "text": "I love watching football and learning about football, with most of my working time spent listening to various NFL podcasts and the Learning Bayesian Stats podcast.1 For the last few years, I have had an inherent fascination with Bayesian stats. One of the great things about Bayesian stats is that we can talk about uncertainty in a more intuitive way, and because of the mechanics of using Bayesian models, we can get some awesome-looking plots.2 So combining these two interests would make for an interesting blog post.\nI got interested in Alex Andora and Maximilian Göbel’s Soccer Factor Model.3 They extend a common model in asset pricing called the factor model to assess player skill. The general idea behind the factor model is that there are lots of macroeconomic variables that affect an asset. To assess the asset or the skill of the broker, we can assess the value of the asset or the broker by adjusting for these variables. Once we have adjusted for these variables, we can just look at the intercept and get our estimand of interest. Alex and Max extend this idea to soccer, where we are adjusting for macro-factors that affect the team. Whatever remains is attributable to a player’s individual skill. They focus on goal scoring as a measurable aspect of a player’s latent skill.\n\n\nCode\nlibrary(patchwork)\nlibrary(arrow)\nlibrary(ggdist)\nlibrary(nflreadr)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(MetBrewer)\nlibrary(reticulate)\nknitr::opts_chunk$set(fig.pos = 'H')\n\n# normally this lives in library(AllenMisc)\n# but fighting renv is exhausting\nsource('scripts/theme-allen-minimal.R')\n\nset_theme(theme_allen_minimal())\n\ngames_dat = rvest::read_html(\n  'https://en.wikipedia.org/wiki/NFL_regular_season'\n) |&gt;\n  rvest::html_element(\n    '#mw-content-text &gt; div.mw-content-ltr.mw-parser-output &gt; table:nth-child(19)'\n  ) |&gt;\n  rvest::html_table()\n\ngames_clean = games_dat |&gt;\n  janitor::clean_names() |&gt;\n  mutate(\n    number_of_games = str_extract(\n      number_of_regular_season_games_per_team_2,\n      '\\\\d{2}'\n    )\n  ) |&gt;\n  slice(16:19) |&gt;\n  mutate(number_of_games = sum(as.numeric(number_of_games))) |&gt;\n  distinct(number_of_games) |&gt;\n  pull(number_of_games)\n\nempirical_dat = read_parquet(here::here('writeup-dat', 'cleaned-data.parquet'))\n\n\nsummary_stats = empirical_dat |&gt;\n  summarise(\n    avg_receiving = mean(receiving_yards),\n    avg_targets = mean(targeted)\n  ) |&gt;\n  mutate(score = (avg_targets * 0.5) + (avg_receiving * 0.1)) |&gt;\n  pull(score)\n\n\n# py_install(c('matplotlib', 'preliz', 'pymc', 'polars', 'pandas', 'pymc'))\n\n\n\n\nCode\nimport preliz as pz \nimport pymc as pm \nimport matplotlib.pyplot as plt \nimport polars as pl\nimport pandas as pd \nimport pytensor.tensor as pt\nfrom scipy.stats import norm\nimport arviz as az\nimport numpy as np\nimport seaborn as sns\nimport xarray as xr\n\nseed = sum(map(ord, \"receivingyardsproject\"))\nrng = np.random.default_rng(seed)\n\n\nplt.style.use('matplot-style.mplstyle')\n\n\n\n\n\n\n\n\n\n\nFigure 1: Touchdowns v.s. Goals\n\n\n\n\n\nAfter reading through the example notebooks and the paper, I thought that this was not only an interesting idea, but it would probably have a strong crossover with touchdowns in football. In Figure 1 I plot a comparison of receiving touchdowns in nflreadr versus goals in the author’s data. The data that are used in the goals plot are a subset of the data that they use in their Sloan analytics paper. From my very limited knowledge of Soccer most o,f these guys seem pretty good, so we are likely to see more players with one goal. Whereas in the NFL play-by-play data, we have a full accounting of every touchdown scored from 2002-2024.4"
  },
  {
    "objectID": "blog/2025/passing-td-factor-model/index.html#introduction",
    "href": "blog/2025/passing-td-factor-model/index.html#introduction",
    "title": "Modeling Receiver Ability Bayesiansly",
    "section": "",
    "text": "I love watching football and learning about football, with most of my working time spent listening to various NFL podcasts and the Learning Bayesian Stats podcast.1 For the last few years, I have had an inherent fascination with Bayesian stats. One of the great things about Bayesian stats is that we can talk about uncertainty in a more intuitive way, and because of the mechanics of using Bayesian models, we can get some awesome-looking plots.2 So combining these two interests would make for an interesting blog post.\nI got interested in Alex Andora and Maximilian Göbel’s Soccer Factor Model.3 They extend a common model in asset pricing called the factor model to assess player skill. The general idea behind the factor model is that there are lots of macroeconomic variables that affect an asset. To assess the asset or the skill of the broker, we can assess the value of the asset or the broker by adjusting for these variables. Once we have adjusted for these variables, we can just look at the intercept and get our estimand of interest. Alex and Max extend this idea to soccer, where we are adjusting for macro-factors that affect the team. Whatever remains is attributable to a player’s individual skill. They focus on goal scoring as a measurable aspect of a player’s latent skill.\n\n\nCode\nlibrary(patchwork)\nlibrary(arrow)\nlibrary(ggdist)\nlibrary(nflreadr)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(MetBrewer)\nlibrary(reticulate)\nknitr::opts_chunk$set(fig.pos = 'H')\n\n# normally this lives in library(AllenMisc)\n# but fighting renv is exhausting\nsource('scripts/theme-allen-minimal.R')\n\nset_theme(theme_allen_minimal())\n\ngames_dat = rvest::read_html(\n  'https://en.wikipedia.org/wiki/NFL_regular_season'\n) |&gt;\n  rvest::html_element(\n    '#mw-content-text &gt; div.mw-content-ltr.mw-parser-output &gt; table:nth-child(19)'\n  ) |&gt;\n  rvest::html_table()\n\ngames_clean = games_dat |&gt;\n  janitor::clean_names() |&gt;\n  mutate(\n    number_of_games = str_extract(\n      number_of_regular_season_games_per_team_2,\n      '\\\\d{2}'\n    )\n  ) |&gt;\n  slice(16:19) |&gt;\n  mutate(number_of_games = sum(as.numeric(number_of_games))) |&gt;\n  distinct(number_of_games) |&gt;\n  pull(number_of_games)\n\nempirical_dat = read_parquet(here::here('writeup-dat', 'cleaned-data.parquet'))\n\n\nsummary_stats = empirical_dat |&gt;\n  summarise(\n    avg_receiving = mean(receiving_yards),\n    avg_targets = mean(targeted)\n  ) |&gt;\n  mutate(score = (avg_targets * 0.5) + (avg_receiving * 0.1)) |&gt;\n  pull(score)\n\n\n# py_install(c('matplotlib', 'preliz', 'pymc', 'polars', 'pandas', 'pymc'))\n\n\n\n\nCode\nimport preliz as pz \nimport pymc as pm \nimport matplotlib.pyplot as plt \nimport polars as pl\nimport pandas as pd \nimport pytensor.tensor as pt\nfrom scipy.stats import norm\nimport arviz as az\nimport numpy as np\nimport seaborn as sns\nimport xarray as xr\n\nseed = sum(map(ord, \"receivingyardsproject\"))\nrng = np.random.default_rng(seed)\n\n\nplt.style.use('matplot-style.mplstyle')\n\n\n\n\n\n\n\n\n\n\nFigure 1: Touchdowns v.s. Goals\n\n\n\n\n\nAfter reading through the example notebooks and the paper, I thought that this was not only an interesting idea, but it would probably have a strong crossover with touchdowns in football. In Figure 1 I plot a comparison of receiving touchdowns in nflreadr versus goals in the author’s data. The data that are used in the goals plot are a subset of the data that they use in their Sloan analytics paper. From my very limited knowledge of Soccer most o,f these guys seem pretty good, so we are likely to see more players with one goal. Whereas in the NFL play-by-play data, we have a full accounting of every touchdown scored from 2002-2024.4"
  },
  {
    "objectID": "blog/2025/passing-td-factor-model/index.html#touchdowns-and-factors",
    "href": "blog/2025/passing-td-factor-model/index.html#touchdowns-and-factors",
    "title": "Modeling Receiver Ability Bayesiansly",
    "section": "Touchdowns and Factors",
    "text": "Touchdowns and Factors\nWhy touchdowns? Outside of being a fun exercise to see how a model designed for soccer translates to football, I think there is at least a reasonable football story for why we can use touchdowns as an outcome and why this is a useful exercise. A lot of people play fantasy football including myself, and want to know who has some scoring upside. Touchdowns in Fantasy Football act as mouthwash for fantasy scoring. I am in a Half Points per Reception league, meaning that a reception is worth 0.5 points, each receiving yard is worth 0.1 points, and a receiving touchdown is worth six points. So, for a league average receiving performance with no touchdowns, this is worth 5.77 fantasy points. A touchdown, or two, turns a baddish fantasy football performance into a relatively good one.\nOutside of my pretend team, TDs could also serve to tap a pass catcher’s latent ability. To be a good pass catcher in the NFL, you have to combine being a good route runner, athleticism, and the ability to catch the ball. You could argue that being a good pass catcher becomes more difficult when a team gets closer to the endzone. If we use some real player tracking data provided by the NFL we can see some of the difficulties of being a receiver in the endzone. If we look at the person who actually catches the touchdown, there are three defenders in the area if we count the corner playing Tyreek Hill. If we turn our attention to the outlet pass (number 35 in red) there are three defenders in the area when he slips out to make himself available. \nUndoubtedly, the probability of scoring increases as the offense gets closer to the endzone, but you have less room to get open. There is a pretty good case that as we start to shrink the field you have to be a crisper route runner and a good pass catcher since space is more limited. During a scramble drill you have to have a feel for where the defense is and your QB’s arm strength. If your QB is slightly off you are likely going to have to make a contested catch because everybody is a lot of closer. One small caveat is that I don’t understand all the nuances of play design and designing an offense, but you could imagine that it matters. As a play designer, you need to think about how to keep defenders where you want them. In the play above, Charcandrick West (number 35) likely has two duties. I would imagine that he is the answer if nobody is open, and he is likely tasked with occupying the linebackers so they do not sink into coverage, closing the window for number 84.\nEven with an explosive play at the edge of the redzone space is still at a premium. Let’s look at this touchdown pass to Tyreek Hill where his skill as a receiver is on display. If we look at the highlight of this play the corner tries to disrupt the route by jamming Hill at the line. Hill avoids the jam and uses his speed to outrun Shields and catches the ball even with Shields right in his face. This is to say, even though there is a higher probability of scoring, your skills as a receiver are extenuated because of the tight quarters.\n\nObviously, we measure wide receiver production in a lot of different ways. Some of the most obvious alternative measurements are efficiency metrics like yards per route run, usage statistics like target share or targets per route run, or just modeling production whether this is receiving yards or yards after catch. In fairness to the nflreadr team they do have this data. Another potential alternative is trying to estimate separation score to measure how they are developing as a route runner. You could totally model this data using offensive personnel as one of the groups, then model your yards metric of choice. However, in my wildest dreams, I would like to use this model throughout the season to inform fantasy football decisions. The participation data that is provided is fantastic, but you must wait till the end of the season. This is likely going to be a future exercise for me.\nTo fit the model, I want we are going to have to leverage information we have before the game. Mainly, some measure of the receiver’s passing offense, how good the defense they are playing is, how fresh the player is, some measure of form, their aging curve, weather, and what kind of game we think it is going to be. In essence, we are adjusting for factors that are going to affect the receiver and the probability of touchdowns. The covariates I use in the model are:\n\nA difference between the defensive team’s passing EPA and the pass catcher’s team EPA.\nThe rest differential between the receiver’s team and the opposition.\nAir yards per pass\nWeather: Mainly wind and temperature.\nTotal line: a combination of both teams’ projected points according to Vegas\nFour binary indicators: Whether the receiver is playing a home game, whether the game is played inside, a division game, or if it is post-2018\n\nI use the total line as a proxy for what kind of game Vegas thinks it is going to be. Effectively I am trying to tap what we think the game script is going to be going into the game. If we think it is going to be a high-scoring game, then this forces one or both teams to rely on a more run-heavy script to keep the opposing offense off the field. I include the difference between the defensive team’s passing EPA per game and the pass catcher team’s passing EPA per game. Effectively I am trying to adjust for how much better the opposing team’s defense is playing going into the matchup. I also include weather and surface as potential confounders. If it is windy and rainy and it is outdoors we are probably not going to see a ton of passes because the ball is harder to throw and catch. I also include whether it is a division game to capture a team’s familiarity with each other.\nThe post-2018 indicator probably deserves a little more exposition. In 2018, the NFL introduced a series of new rules, in part, to promote passing. The big change was a revision to the catch rules to try to eliminate some notably controversial calls. A catch happens when a receiver establishes themselves in bounds and performs a “football move.” Additionally, the ball is allowed to move if it is in the receiver’s control. In the clip below Brandon Aiyuk makes a great catch where the ball moves during the play. Before 2018 this likely would have been ruled an incomplete catch and the 49ers would have likely kicked a field goal.\n\n\nTo model time, I make use of Hilbert Space Gaussian Processes (HSGP). Most of the textbook definitions of a Gaussian Process (GP) start with the idea that this is a wholly uninformative name. Effectively, a Gaussian process is a collection of random variables where any finite subset has a Gaussian distribution. It is effectively just an infinite vector a.k.a a function where we are going to place a prior over. Generally, Gaussian processes are used to model time or space or both. Mathematically, this involves a lot of matrix inversion to get the posterior covariance. What this means practically is that the execution is \\(O(n^3)\\) to get a sense of what that means, I plotted how long it would take to fit a single Gaussian process. Game level NFL data is not necessarily all that big but there are about 2080 games in the nflreadr database, without including the play-by-play data where we are including data from just about every wide receiver to take a snap. To get around having to wait 30+ hours to fit a model we can use a lower level approximation of GP known as a HSGP. We are using an approximation of a GP where we use basis to capture the wiggliness of the function while basically converting everything from a matrix inversion to matrix multiplication which is a much faster operation.\n\n\n\n\n\n\n\n\n\nWe are interested in modeling two different time components that don’t have an obvious functional form. The first is modeling how well a player is playing in a particular season. They could be having an awesome season, and that is carrying over from game to game because they are being used more appropriately in the scheme or their usage has changed. More critically, we are interested in how experience impacts ability. In the most optimistic case you get a 21-year-old rookie into your building and in year one, they are at or above league average, but have some maturing to do with the finer aspects of being a pass catcher. By the time they get to their second contract, they may not be as fast as they were coming out of college, but they are an overall better pass catcher. Then, towards the end of their career they dip back to where they were as a rookie because they have taken a step back athletically.\nThis is a linearish story of receiver ability and a player’s ability in general is one that fanbases, GMs, and coaches would sign up for immediately, but it rarely ever happens. Tight End has a big jump from college to the NFL for a variety of reasons. George Kittle is a great example of the diversity of responsibilities that an elite tight end has in the NFL. Part of what makes him elite is that he is an awesome blocker who can be used at the point of attack. Sometimes this includes blocking a team’s best edge rusher, which is a difficult task for elite tackles, never mind a Tight End. To alleviate some of the difficulty, Shanahan uses a lot of motion to try and create advantageous angles and head starts. The rub is that how the motion and blocking look on a run play should look the same as when he is used on play action. As you can imagine, this is difficult, especially when you are just getting used to the size and speed of an NFL defender and the complexity of the NFL.\nTravis Kelce is another great example of the difficulty of being a pass catcher in the NFL. Over the years, Kelce has built a big reputation for his improvisation in route running.5 A lot of the plays that get dialed up for him are choice routes where he can decide on what route to run based on the coverage. You can run what is known as “pause and bounce,” where the pass catcher “misses the count” where you are deliberately a tick slow. To combat under-center play, action defenses will change the picture after the snap or switch coverage. By delaying your route, you can get more information about the coverage to run your route. This takes a lot of preparation and experience to execute. This maturation process is likely not linear and is not going to have the same effect on every player. At the same time, we don’t really expect a mostly blocking tight end to suddenly catch fire as a scoring threat."
  },
  {
    "objectID": "blog/2025/passing-td-factor-model/index.html#the-fun-stuff-modeling-the-data",
    "href": "blog/2025/passing-td-factor-model/index.html#the-fun-stuff-modeling-the-data",
    "title": "Modeling Receiver Ability Bayesiansly",
    "section": "The Fun Stuff: Modeling the Data",
    "text": "The Fun Stuff: Modeling the Data\nI fit an Ordered logit for each player for each player i in game g within each season s. The rough sketch of the model takes this form. For a more detailed look at the data collection, data cleaning, and modeling files, I will point you towards the files in the script folder. The sandbox folder is really a way for me to play around the various aspects of tuning the model. \\[\n\\begin{aligned}\n\\ell_{experience},\\ell_{form} \\sim InverseGamma(\\alpha, \\beta) \\\\\n\\sigma_{experience}, \\sigma_{form} \\sim Exponential(\\lambda) \\\\\n\\beta_{factor} \\sim \\mathcal{N}(\\mu_{factors}, \\sigma_{factor}^{2}), k = 1, \\ldots, p \\\\\n\\sigma_{player} \\sim Exponential(1) \\\\\n\\sigma_{baseline} \\sim \\sqrt{\\sigma^2_{player} + \\frac{\\sigma^2_{cutpoints}}{J}} \\\\\n\\beta_{0} ~ \\mathcal{N}(0, \\sigma^2_{baseline}) \\\\\n\\alpha_j = \\beta_{0} + alpha_{j}^{raw}, where \\sum^j_{j=1}\\alpha^{raw}_i=0, \\alpha^[raw]_{j} \\sim \\mathcal{N}(0, \\sigma^2_{i}) \\\\\nf_{experience}(s) \\sim \\mathcal{GP}(0, \\sigma^2_{experience} \\cdot K_{Matérn}(\\cdot, \\cdot;\\ell_{experience})) \\\\\nf_{performance}(g) \\sim \\mathcal{GP}(0, \\sigma^2_{performance} \\cdot K_{Matérn}(\\cdot, \\cdot;\\ell_{performance})) \\\\\nN_i = \\alpha_i + f_{experience}(s_i) + f_{performance}(g_i) + \\mathcal{X}^{\\top}_{i}\\beta \\\\\nTouchdowns_{i} \\sim \\text{Ordered Logit}(N_{i}, \\mathcal{c}_{i})\n\\end{aligned}\n\\]\n\nSetting Priors\nAn ordered categorical likelihood seems kind of like a weird fit since we are really just using counts. However, we don’t have a ton of mass in the 3+ touchdown range. Even in the 2+ touchdown range, we are working with even less mass than the goal scoring data that Max and Alex are using. I would imagine if they included attacking midfielders then the counts would look a little more similar. We could use what I like to call “you must be this tall to ride the ride” approach, meaning we could throw out any pass catcher without enough games played or enough targets. However, we may be getting rid of some interesting comparative information when we want to go and calculate replacement-level stats.\n\nempirical_dat |&gt;\n  group_by(rec_tds_game) |&gt;\n  summarise(`Total Touchdowns` = n()) |&gt;\n  tinytable::tt()\n\n\n\nTable 1\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                rec_tds_game\n                Total Touchdowns\n              \n        \n        \n        \n                \n                  0\n                  54862\n                \n                \n                  1\n                  11057\n                \n                \n                  2\n                  1477\n                \n                \n                  3\n                  147\n                \n                \n                  4\n                  15\n                \n                \n                  6\n                  1\n                \n        \n      \n    \n\n\n\n\n\n\nAdditionally, while there is no technical upper bound to the number of touchdowns you could score in a game or a season there are some practical bounds on the total number of touchdowns. The current single-season record is held by Randy Moss with 23, a record that is 18 years old, which broke Jerry Rice’s single-season record of 22, which was 20. The current single-game record is a three-way tie between Kellen Winslow, Bob Shaw, and Jerry Rice, with each player having 5 receiving touchdowns in a single game. No receiver since Jerry Rice in 1990 has had 5 receiving touchdowns in a game.6\n\nhist_dat = empirical_dat |&gt;\n  mutate(binary = ifelse(rec_tds &gt;= 1, 1, 0)) |&gt;\n  pivot_longer(c(rec_tds, rec_tds_game, binary)) |&gt;\n  mutate(\n    name = case_when(\n      name == 'rec_tds_game' ~ 'Observed TDs',\n      name == 'rec_tds' ~ 'Lumped TDs',\n      name == 'binary' ~ 'Indicator TDs'\n    )\n  )\n\nggplot(hist_dat, aes(x = value, fill = name)) +\n  geom_bar(alpha = 0.5, position = 'dodge') +\n  labs(x = 'Touchdowns', y = 'Count', fill = NULL) +\n  MetBrewer::scale_fill_met_d(name = 'Lakota') +\n  scale_y_continuous(labels = scales::comma)\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nInstead of using the full observed range, I am just going to lump together 3 touchdowns and 4 touchdowns together. Functionally, nothing really changes because 3 touchdowns are still a relatively rare occurrence. Even when we create a simple binary indicator, we are not really changing things too much. I decided to use an ordered logit because a two or three-touchdown game is still useful for understanding how much better a pass catcher is than league average. Generally, the great pass catchers have multiple touchdown games. There are some games where a semi-random pass catcher may have a multiple TD game, but these are few and far between. Kyle Juszczyk has been an excellent receiving fullback in his career. However, he is not necessarily a major scoring threat, with only one game where he has scored multiple receiving touchdowns.\nThe biggest difference that I found when changing the goal scoring model to the touchdown scoring model was dealing with time. The soccer season is considerably longer than the football schedule, with 38 matchdays, while the length of the football season ranges from 14-17 games over the course of NFL games. Fitting two GPs into one season is feasible but a little bit overkill. Careers in the NFL also tend to be a bit shorter than in European high-level soccer. In general, an NFL career is 3ish years whereas the career lengths in European high-level soccer are longer because there are more avenues available to develop a player.\nTechnically, when you are talking about a lengthscale in general, we are talking about setting the priors over how quickly the correlations between function values decay. One of the nice things about the PyMC universe is that they have made setting the priors and hyperparameters of an HSGP more intuitive. So, while thinking about the prior for the variable may technically be a bit wrong in practice, it was helpful to do it when setting priors for the in-season HSGP and the experience HSGP.\nFor the in-season prior I started by thinking about how much carryover we would expect from game to game. For my mental model I found it easier to just lop off the last week of the season, since players may not be playing because of injuries or because playoff seeding is more or less set by then, so they are not playing. My intuition about how a player is playing carries over for a max of 5 or 6 games, while their performance from the last two weeks is going to tell us a bit more about how they are going to do in their next game. As the seasons evolve good to good-to-goodish teams tend to start to find answers to their problems. Mentally, I think this is kind of saying half of the season is going to tell you how a player will perform in that half of the season.\n\n# | echo: false\n# | message: false\n# | warning: false\n# | label: fig-short-term-form-prior\n\nshort_term_form, ax = pz.maxent(pz.InverseGamma(), lower=2, upper=5)\nlg = ax.legend()\nax.set_xlim(0, 18);\n\n\n\n\n\n\n\n\nFor the season’s GP, this was a bit more of a challenge because you don’t need to be a consistent scoring threat to be an important player in the offense. Because I am not sub-setting the data to exclude players with a certain number of targets, I end up also including some blocking TEs and Fullbacks that could probably be dropped. Intuitively, this means that we probably have some players who pull the average career length down. Partially because teams may be looking for more juice at these positions and are more likely to move on from veterans. I\nI try to put the center of the distribution around 3-8 seasons. For the most part NFL careers are about 3 years long, so the first 1-3 years are probably going to be pretty informative. In general, good wide receivers get a new contract around their \\(4^{th}\\) or \\(5^{th}\\) year due to how the collective bargaining agreement works. By their second contract they are around 25-26, and the team and the league know what they are as a pass catcher.\n\n\n\n\nTable 2\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        Average Age When Drafted\n        \n              \n                Receiver Position\n                Average Age\n              \n        \n        \n        \n                \n                  WR\n                  22.52\n                \n                \n                  RB\n                  22.46\n                \n                \n                  TE\n                  22.79\n                \n        \n      \n    \n\n\n\n\n\n\nBy their third contract they are not only expensive but they are starting to decline athletically so what the first half of their may not be as informative. Players like Mike Evans, Larry Fitzgerald, and Davante Adams who relied on their route running to be dominant may have a longer tail because they can remain productive on a third contract. What this amounts to is a prior that looks like this.\n\n\n(0.0, 18.0)\n\n\n\n\n\n\n\n\n\nThe next prior we need to set is how much ability varies from player to player. Admittedly this is a little bit more difficult for me to conceptualize. The difference between a league-average receiver and the best receiver in the NFL is pretty big. Where I struggled was mostly because initially I was setting it at a season level, so the difference between say WR1 and WR2 is probably closer to 2 or 3 touchdowns. However, this is being set at the game’s level so realistically, the difference is probably closer to a touchdown. So the prior level differences are going to look closer to something like this.\n\n\n\n\n\n\n\n\n\nThe variance for the HSGPs are set by really looking at the observed proportions of TDs. I set it by looking at the proportion of 2-touchdown games. So, roughly what is the proportion of two-touchdown games we would expect to observe? In the actual data, there is about a 2% chance of a player having a two-touchdown game. However, during the model-building process, I found that setting the prior at 2% or 2.1% was causing the model to struggle sampling a bit. I ended up setting the prior to 3% and that seemed to help with sampling.\n\nempirical_dat |&gt;\n  summarise(tds = n(), .by = rec_tds) |&gt;\n  mutate(\n    `Observed Receiving TD Proportion` = round(tds / sum(tds), 3),\n    rec_tds = as.character(rec_tds),\n    rec_tds = ifelse(rec_tds == '3', '3+', rec_tds)\n  ) |&gt;\n  arrange(rec_tds) |&gt;\n  select(\n    `Receiving Touchdowns` = rec_tds,\n    `Observed Receiving TD Proportion`\n  ) |&gt;\n  tinytable::tt()\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                Receiving Touchdowns\n                Observed Receiving TD Proportion\n              \n        \n        \n        \n                \n                  0\n                  0.812\n                \n                \n                  1\n                  0.164\n                \n                \n                  2\n                  0.022\n                \n                \n                  3+\n                  0.002\n                \n        \n      \n    \n\n\n\n\n\nPrior Predictive simulations\nBefore we introduce our model to the data lets go and look at what our predictions from the model will look like.7\n\n\nCode\ncumulative_stats = pl.read_parquet(\"writeup-dat/cleaned-data.parquet\")\n\nfactors_numeric = [\n    \"player_rest_diff\",\n    \"def_epa_diff\",\n    \"wind\",\n    \"temp\",\n    \"total_line\",\n    \"air_yards_per_pass_attempt\",\n]\n\nfactors = factors_numeric + [\"div_game\", \"home_game\", \"is_indoors\", \"era\"]\n\nfactors_numeric_train = cumulative_stats.select(pl.col(factors))\n\nmeans = factors_numeric_train.select(\n    [pl.col(c).mean().alias(c) for c in factors_numeric]\n)\nsds = factors_numeric_train.select([pl.col(c).std().alias(c) for c in factors_numeric])\n\nfactors_numeric_sdz = factors_numeric_train.with_columns(\n    [((pl.col(c) - means[0, c]) / sds[0, c]).alias(c) for c in factors_numeric]\n).with_columns(\n    pl.Series(\"home_game\", cumulative_stats[\"home_game\"]),\n    pl.Series(\"div_game\", cumulative_stats[\"div_game\"]),\n    pl.Series(\"is_indoors\", cumulative_stats[\"is_indoors\"]),\n    pl.Series(\"era\", cumulative_stats[\"era\"]),\n)\n\ncumulative_stats_pd = cumulative_stats.to_pandas()\n\n\nunique_games = cumulative_stats_pd[\"games_played\"].sort_values().unique()\nunique_seasons = cumulative_stats_pd[\"number_of_seasons_played\"].sort_values().unique()\n\noff_play_caller = cumulative_stats_pd[\"off_play_caller\"].sort_values().unique()\ndef_play_caller = cumulative_stats_pd[\"def_play_caller\"].sort_values().unique()\n\nunique_players = cumulative_stats_pd[\"receiver_full_name\"].sort_values().unique()\n\n\nplayer_idx = pd.Categorical(\n    cumulative_stats_pd[\"receiver_full_name\"], categories=unique_players\n).codes\n\nseasons_idx = pd.Categorical(\n    cumulative_stats_pd[\"number_of_seasons_played\"], categories=unique_seasons\n).codes\n\ngames_idx = pd.Categorical(\n    cumulative_stats_pd[\"games_played\"], categories=unique_games\n).codes\n\noff_play_caller_idx = pd.Categorical(\n    cumulative_stats_pd[\"off_play_caller\"], categories=off_play_caller\n).codes\n\ndef_play_caller_idx = pd.Categorical(\n    cumulative_stats_pd[\"def_play_caller\"], categories=def_play_caller\n).codes\n\ncoords = {\n    \"factors\": factors,\n    \"gameday\": unique_games,\n    \"seasons\": unique_seasons,\n    \"obs_id\": cumulative_stats_pd.index,\n    \"player\": unique_players,\n    \"off_play_caller\": off_play_caller,\n    \"def_play_caller\": def_play_caller,\n    \"time_scale\": [\"games\", \"season\"],\n}\n\nempirical_probs = cumulative_stats_pd[\"rec_tds\"].value_counts(normalize=True).to_numpy()\n\ncumulative_probs = empirical_probs.cumsum()[:-1]\n\ncutpoints_standard = norm.ppf(cumulative_probs)\n\ndelta_prior = np.diff(cutpoints_standard)\n\nseasons_m, seasons_c = pm.gp.hsgp_approx.approx_hsgp_hyperparams(\n    x_range=[\n        0,\n        cumulative_stats.select(pl.col(\"number_of_seasons_played\").max()).to_series()[\n            0\n        ],\n    ],\n    lengthscale_range=[2, 6],\n    cov_func=\"matern52\",\n)\n\n\nshort_term_form, _ = pz.maxent(pz.InverseGamma(), lower=2, upper=5)\n\n\nwithin_m, within_c = pm.gp.hsgp_approx.approx_hsgp_hyperparams(\n    x_range=[\n        0,\n        cumulative_stats.select(pl.col(\"games_played\").max()).to_series()[0],\n    ],\n    lengthscale_range=[2, 5],\n    cov_func=\"matern52\",\n)\n\n\nwith pm.Model(coords=coords) as rec_tds_era_adjusted:\n    factor_data = pm.Data(\"factor_data\", factors_numeric_sdz, dims=(\"obs_id\", \"factor\"))\n    games_id = pm.Data(\"games_id\", games_idx, dims=\"obs_id\")\n    player_id = pm.Data(\"player_id\", player_idx, dims=\"obs_id\")\n    season_id = pm.Data(\n        \"season_id\",\n        seasons_idx,\n        dims=\"obs_id\",\n    )\n\n    rec_tds_obs = pm.Data(\n        \"rec_tds_obs\", cumulative_stats[\"rec_tds\"].to_numpy(), dims=\"obs_id\"\n    )\n\n    x_gamedays = pm.Data(\"x_gamedays\", unique_games, dims=\"gameday\")[:, None]\n    x_seasons = pm.Data(\"x_seasons\", unique_seasons, dims=\"seasons\")[:, None]\n\n    # ref notebook sets it at the max of goals scored of the games so we are going to do the same\n    intercept_sigma = 4\n    sd = touchdown_dist.to_pymc(\"touchdown_sd\")\n\n    baseline_sigma = pt.sqrt(intercept_sigma**2 + sd**2 / len(coords[\"player\"]))\n\n    baseline = baseline_sigma * pm.Normal(\"baseline\")\n\n    player_effect = pm.Deterministic(\n        \"player_effect\",\n        baseline + pm.ZeroSumNormal(\"player_effect_raw\", sigma=sd, dims=\"player\"),\n        dims=\"player\",\n    )\n\n    # bumbing this up a bit\n    alpha_scale, upper_scale = 0.03, 2.0\n    gps_sigma = pm.Exponential(\n        \"gps_sigma\", lam=-np.log(alpha_scale) / upper_scale, dims=\"time_scale\"\n    )\n\n    ls = pm.InverseGamma(\n        \"ls\",\n        alpha=np.array([short_term_form.alpha, seasons_gp_prior.alpha]),\n        beta=np.array([short_term_form.beta, seasons_gp_prior.beta]),\n        dims=\"time_scale\",\n    )\n\n    cov_games = gps_sigma[0] ** 2 * pm.gp.cov.Matern52(input_dim=1, ls=ls[0])\n    cov_seasons = gps_sigma[1] ** 2 * pm.gp.cov.Matern52(input_dim=1, ls=ls[1])\n\n    gp_games = pm.gp.HSGP(m=[within_m], c=within_c, cov_func=cov_games)\n    gp_season = pm.gp.HSGP(m=[seasons_m], c=seasons_c, cov_func=cov_seasons)\n\n    basis_vectors_game, sqrt_psd_game = gp_games.prior_linearized(X=x_gamedays)\n\n    basis_coeffs_games = pm.Normal(\"basis_coeffs_games\", shape=gp_games.n_basis_vectors)\n\n    f_games = pm.Deterministic(\n        \"f_games\",\n        basis_vectors_game @ (basis_coeffs_games * sqrt_psd_game),\n        dims=\"gameday\",\n    )\n\n    basis_vectors_season, sqrt_psd_season = gp_season.prior_linearized(X=x_seasons)\n\n    basis_coeffs_season = pm.Normal(\n        \"basis_coeffs_season\", shape=gp_season.n_basis_vectors\n    )\n\n    f_season = pm.Deterministic(\n        \"f_season\",\n        basis_vectors_season @ (basis_coeffs_season * sqrt_psd_season),\n        dims=\"seasons\",\n    )\n\n    alpha = pm.Deterministic(\n        \"alpha\",\n        player_effect[player_id] + f_season[season_id] + f_games[games_id],\n        dims=\"obs_id\",\n    )\n    slope = pm.Normal(\"slope\", sigma=0.5, dims=\"factors\")\n\n    eta = pm.Deterministic(\n        \"eta\", alpha + pm.math.dot(factor_data, slope), dims=\"obs_id\"\n    )\n    cutpoints_off = 4\n\n    delta_mean = pm.Normal(\n        \"delta_mean\", mu=delta_prior * cutpoints_off, sigma=1, shape=2\n    )\n\n    delta_sig = pm.Exponential(\"delta_sig\", 1, shape=2)\n\n    player_delta = delta_mean + delta_sig * pm.Normal(\n        \"player_delta\", shape=(len(coords[\"player\"]), 2)\n    )\n\n    cutpoints = pm.Deterministic(\n        \"cutpoints\",\n        pt.concatenate(\n            [\n                pt.full((player_effect.shape[0], 1), cutpoints_off),\n                pt.cumsum(pt.softplus(player_delta), axis=-1) + cutpoints_off,\n            ],\n            axis=-1,\n        ),\n    )\n\n    pm.OrderedLogistic(\n        \"tds_scored\",\n        cutpoints=cutpoints[player_id],\n        eta=eta,\n        observed=rec_tds_obs,\n        dims=\"obs_id\",\n    )\n\n\nwith rec_tds_era_adjusted:\n    idata = pm.sample_prior_predictive()\n\n\n\n\n\n\n\n\n\nLet’s check how the HSGPs are looking. Admittedly, we do get some pretty wonky-looking lines, but most of them are in the High Density intervals. It would be nice to see less wonky-looking lines from the simulations.\n\n\nCode\nf_within_prior = idata.prior[\"f_games\"]\nf_long_prior = idata.prior[\"f_season\"]\n\nindex = pd.MultiIndex.from_product(\n    [unique_seasons, unique_games],\n    names=[\"season_nbr\", \"gameday\"],\n)\nunique_combinations = pd.DataFrame(index=index).reset_index()\n\nf_long_prior_aligned = f_long_prior.sel(\n    seasons=unique_combinations[\"season_nbr\"].to_numpy()\n).rename({\"seasons\": \"timestamp\"})\nf_long_prior_aligned[\"timestamp\"] = unique_combinations.index\n\nf_within_prior_aligned = f_within_prior.sel(\n    gameday=unique_combinations[\"gameday\"].to_numpy()\n).rename({\"gameday\": \"timestamp\"})\nf_within_prior_aligned[\"timestamp\"] = unique_combinations.index\n\nf_total_prior = f_long_prior_aligned + f_within_prior_aligned\n\nsome_draws = rng.choice(f_total_prior.draw, size=20, replace=True)\n\n_, axes = plt.subplot_mosaic(\n    \"\"\"\n    AB\n    CC\n    \"\"\",\n    figsize=(12, 7.5),\n    layout=\"constrained\",\n)\n\naxes[\"A\"].plot(\n    f_within_prior.gameday,\n    az.extract(f_within_prior)[\"f_games\"].isel(sample=0),\n    color=\"#70133A\",\n    alpha=0.3,\n    lw=1.5,\n    label=\"random draws\",\n)\naxes[\"A\"].plot(\n    f_within_prior.gameday,\n    az.extract(f_within_prior)[\"f_games\"].isel(sample=some_draws),\n    color=\"#70133A\",\n    alpha=0.3,\n    lw=1.5,\n)\naz.plot_hdi(\n    x=f_within_prior.gameday,\n    y=f_within_prior,\n    hdi_prob=0.83,\n    color=\"#AAC4E6\",\n    fill_kwargs={\"alpha\": 0.9, \"label\": r\"$83\\%$ HDI\"},\n    ax=axes[\"A\"],\n    smooth=False,\n)\naxes[\"A\"].plot(\n    f_within_prior.gameday,\n    f_within_prior.mean((\"chain\", \"draw\")),\n    color=\"#FBE64D\",\n    lw=2.5,\n    label=\"Mean\",\n)\naxes[\"A\"].set(\n    xlabel=\"Gameday\", ylabel=\"Nbr TDs\", title=\"Within season variation\\nForm GP\"\n)\naxes[\"A\"].legend(fontsize=10, frameon=True, ncols=3)\n\naxes[\"B\"].plot(\n    f_long_prior.seasons,\n    az.extract(f_long_prior)[\"f_season\"].isel(sample=some_draws),\n    color=\"#70133A\",\n    alpha=0.3,\n    lw=1.5,\n)\naz.plot_hdi(\n    x=f_long_prior.seasons,\n    y=f_long_prior,\n    hdi_prob=0.83,\n    color=\"#AAC4E6\",\n    fill_kwargs={\"alpha\": 0.9},\n    ax=axes[\"B\"],\n    smooth=False,\n)\naxes[\"B\"].plot(\n    f_long_prior.seasons,\n    f_long_prior.mean((\"chain\", \"draw\")),\n    color=\"#FBE64D\",\n    lw=2.5,\n)\naxes[\"B\"].set(\n    xlabel=\"Season\", ylabel=\"Nbr TDs\", title=\"Across seasons variation\\nAging curve\"\n)\n\naxes[\"C\"].plot(\n    f_total_prior.timestamp,\n    az.extract(f_total_prior)[\"x\"].isel(sample=some_draws),\n    color=\"#70133A\",\n    alpha=0.3,\n    lw=1.5,\n)\naz.plot_hdi(\n    x=f_total_prior.timestamp,\n    y=f_total_prior,\n    hdi_prob=0.83,\n    color=\"#AAC4E6\",\n    fill_kwargs={\"alpha\": 0.9},\n    ax=axes[\"C\"],\n    smooth=False,\n)\naxes[\"C\"].plot(\n    f_total_prior.timestamp,\n    f_total_prior.mean((\"chain\", \"draw\")),\n    color=\"#FBE64D\",\n    lw=2.5,\n)\naxes[\"C\"].set(xlabel=\"Timestamp\", ylabel=\"Nbr TDS\", title=\"Total GP\")\nplt.suptitle(\"Prior GPs\", fontsize=18)\n\n\n\n\n\n\n\n\n\nLet’s go and look at the implied predictions from the model. The model is a little bit more optimistic about players scoring 3 or more touchdowns than we actually observe in the real world. However, it looks like it does a pretty good job of projecting zeros and ones. This good because this is where most of the action is in the data, so I am overall pretty happy about that.\n\n\nCode\nimplied_cats = az.extract(idata.prior_predictive, var_names='tds_scored')\n\n\nfig, axes = plt.subplots(ncols=2)\n\naxes[0] = (\n    implied_cats.isel(obs_id=0)\n    .to_pandas()\n    .reset_index(drop=True)\n    .value_counts(normalize=True)\n    .sort_index()\n    .plot(kind=\"bar\", rot=0, alpha=0.8, ax=axes[0])\n)\naxes[0].set(\n    xlabel=\"Touchdowns\",\n    ylabel=\"Proportion\",\n    title=\"Prior allocation of \\n TDs for Observation 0\",\n)\n\naxes[1] = (\n    cumulative_stats_pd[\"rec_tds\"]\n    .value_counts(normalize=True)\n    .sort_index()\n    .plot(kind=\"bar\", rot=0, alpha=0.8, ax=axes[1])\n)\n\naxes[1].set(\n    xlabel=\"Touchdowns\", ylabel=\"Proportion\", title=\"Observed TDs for Observation 0\"\n)\n\n\n\n\n\n\n\n\n\nWe can see that a little bit more clearly when we visualize the prior predictive distribution. The model underpredicts zero by a bit, while it gets one touchdown pretty close to dead on. Overall, I think the model looks acceptable. We could try and dial everything in a whole lot more, but that feels like playing with fire.\n\n\n\n\n\n\n\n\n\nNow lets look at the GPs to make sure they are\n\n\nDiagnostics\nUnfortunately even with some pretty permissive sampling setting we still get 3 divergences which is not the end of the world but it is not zero.\n\nidata = az.from_netcdf(\n    'models/idata_compelete.nc'\n)\n\nidata.sample_stats.diverging.sum().data\n\nMost if not all of our proposals were accepted which is goo!\n\nidata.sample_stats.acceptance_rate.mean().data.round(2)\n\nThe Rhats for our HSGPs are at or really close to one, indicating that they are sampling pretty well!\n\naz.rhat(\n    idata, var_names=[\"basis_coeffs_season\", \"basis_coeffs_games\"]\n).max().to_pandas().round(2)\n\nPersonally, I would have really liked everybody to be above 1,000. The sigma for the touchdown’s parameter is pretty small ESS of 519, but we are still a bit above the threshold where PyMC will yell at you.\n\n\nCode\ness = read_parquet('writeup-dat/ess.parquet') |&gt;\n  mutate(\n    Variable = str_replace_all(Variable, '_', ' '),\n    Variable = str_remove(Variable, 'basis'),\n    Variable = str_to_title(Variable),\n    Variable = case_when(\n      Variable == 'F Games' ~ 'HSGP Games',\n      Variable == 'F Season' ~ 'HSGP Season',\n      Variable == 'Ls' ~ 'Length Scale',\n      .default = Variable\n    )\n  )\n\n\nggplot(\n  ess,\n  aes(x = ess, y = fct_reorder(Variable, ess, .fun = max), label = ess)\n) +\n  geom_col() +\n  geom_text(hjust = -0.1) +\n  labs(x = 'Effective Sample Size', y = NULL)\n\n\n\n\n\n\n\n\n\nWorking with the posteriors from this model is not necessarily difficult but because there are 2265 with 1,000 draws across four chains and a few parameters that are indexed by player, this can take a while. To speed up compilation of the graphs, I do make some quick and dirty trace plots by taking a random sample of values for each variable across each chain.\n\n\nCode\nposteriors_dat = open_dataset(\n  'trace-plot-data'\n) |&gt;\n  filter(\n    param %in%\n      c(\n        'alpha',\n        'f_season',\n        'f_games',\n        'cutpoint',\n        'eta',\n        'tds_scored_probs',\n        'slope',\n        'player_delta',\n        'player_effect',\n        'slope'\n      )\n  ) |&gt;\n  collect() |&gt;\n  mutate(\n    param = str_replace_all(param, '_', ' '),\n    param = str_to_title(param),\n    param = case_match(\n      param,\n      'F Season' ~ 'HSGP Season',\n      'F Games' ~ 'HSGP Games',\n      .default = param\n    )\n  )\n\n\nggplot(\n  posteriors_dat,\n  aes(x = .iteration, y = value, color = as.factor(.chain))\n) +\n  # drawing points is faster than connecting lines\n  geom_line(alpha = 0.3) +\n  facet_wrap(vars(param), scales = 'free_y', labeller = label_wrap_gen(2)) +\n  MetBrewer::scale_colour_met_d(name = 'Lakota') +\n  theme(\n    legend.position = 'none'\n  )\n\n\n\n\n\n\n\n\n\nFor the most part these look pretty good everything looks wiggly and jumbled, which is generally a good sign for how well the sampler is doing. The slopes and HSGPs are a little less like a jumply mess. For the most part things are looking pretty good.\nMoving on if we look at the posterior predictive distribution, the model does well capturing the observed data. The posterior predictive mean is a little lower than the observed rate of 0’s and 1’s, but you kind of have to squint to be able to tell. The other encouraging sign is that all the posterior predictive draws are clustered right on or at least right near the observed values, and we are not getting some wild, implausible predictions.\n\n\nCode\nidata = az.from_netcdf('models/idata_compelete.nc')\n\nwith rec_tds_era_adjusted:\n    idata.extend(\n        pm.sample_posterior_predictive(idata, compile_kwargs={\"mode\":'NUMBA'})\n    )\n\naz.plot_ppc(idata, num_pp_samples=100)\n\n\n\n\n\n\n\n\n\nNow when we go to look at the posteriors for the GPs we do get a few odd random draws but for the most part all three of the GPs do a good job over their respective ranges which is good!\n\nPythonR\n\n\n\n\nCode\nf_within_posterior = idata.posterior[\"f_games\"]\nf_long_posterior = idata.posterior[\"f_season\"]\n\n#check = f_within_posterior.to_dataframe().reset_index()\n#\n#check2 = f_long_posterior.to_dataframe().reset_index()\n#\n#games_hsgp = pl.from_dataframe(check)\n#\n#games_hsgp.write_parquet('writeup-dat/games-hsgp.parquet')\n\n\nindex = pd.MultiIndex.from_product(\n    [unique_seasons, unique_games],\n    names=[\"season_nbr\", \"gameday\"],\n)\nunique_combinations = pd.DataFrame(index=index).reset_index()\n\nf_long_posterior_aligned = f_long_posterior.sel(\n    seasons=unique_combinations[\"season_nbr\"].to_numpy()\n).rename({\"seasons\": \"timestamp\"})\n\nf_long_posterior_aligned[\"timestamp\"] = unique_combinations.index\n\nf_within_posterior_aligned = f_within_posterior.sel(\n    gameday=unique_combinations[\"gameday\"].to_numpy()\n).rename({\"gameday\": \"timestamp\"})\n\nf_within_posterior_aligned[\"timestamp\"] = unique_combinations.index\n\n\nf_total_posterior = f_long_posterior_aligned + f_within_posterior_aligned\n\n\n\n#total_post = pl.from_dataframe(f_total_posterior.to_dataframe().reset_index()\n#total_post.write_parquet('writeup-dat/total_hsgp.parquet')\n\nsome_draws = rng.choice(f_total_posterior.draw, size=100, replace=True)\n\n_, axes = plt.subplot_mosaic(\n    \"\"\"\n    AB\n    CC\n    \"\"\",\n    figsize=(12, 7.5),\n    layout=\"constrained\",\n)\n\naxes[\"A\"].plot(\n    f_within_posterior.gameday,\n    az.extract(f_within_posterior)[\"f_games\"].isel(sample=0),\n    color=\"#70133A\",\n    alpha=0.3,\n    lw=1.5,\n    label=\"random draws\",\n)\n\n\naxes[\"A\"].plot(\n    f_within_posterior.gameday,\n    az.extract(f_within_posterior)[\"f_games\"].isel(sample=some_draws),\n    color=\"#70133A\",\n    alpha=0.3,\n    lw=1.5,\n)\naz.plot_hdi(\n    x=f_within_posterior.gameday,\n    y=f_within_posterior,\n    hdi_prob=0.83,\n    color=\"#AAC4E6\",\n    fill_kwargs={\"alpha\": 0.9, \"label\": r\"$83\\%$ HDI\"},\n    ax=axes[\"A\"],\n    smooth=False,\n)\naxes[\"A\"].plot(\n    f_within_posterior.gameday,\n    f_within_posterior.mean((\"chain\", \"draw\")),\n    color=\"#FBE64D\",\n    lw=2.5,\n    label=\"Mean\",\n)\naxes[\"A\"].set(\n    xlabel=\"Gameday\", ylabel=\"Nbr TDs\", title=\"Within season variation\\nShort GP\"\n)\naxes[\"A\"].legend(fontsize=10, frameon=True, ncols=3)\n\naxes[\"B\"].plot(\n    f_long_posterior.seasons,\n    az.extract(f_long_posterior)[\"f_season\"].isel(sample=some_draws),\n    color=\"#70133A\",\n    alpha=0.3,\n    lw=1.5,\n)\naz.plot_hdi(\n    x=f_long_posterior.seasons,\n    y=f_long_posterior,\n    hdi_prob=0.83,\n    color=\"#AAC4E6\",\n    fill_kwargs={\"alpha\": 0.9},\n    ax=axes[\"B\"],\n    smooth=False,\n)\naxes[\"B\"].plot(\n    f_long_posterior.seasons,\n    f_long_posterior.mean((\"chain\", \"draw\")),\n    color=\"#FBE64D\",\n    lw=2.5,\n)\naxes[\"B\"].set(\n    xlabel=\"Season\", ylabel=\"Nbr TDs\", title=\"Across seasons variation\\nAging curve\"\n)\n\naxes[\"C\"].plot(\n    f_total_posterior.timestamp,\n    az.extract(f_total_posterior)[\"x\"].isel(sample=some_draws),\n    color=\"#70133A\",\n    alpha=0.3,\n    lw=1.5,\n)\naz.plot_hdi(\n    x=f_total_posterior.timestamp,\n    y=f_total_posterior,\n    hdi_prob=0.83,\n    color=\"#AAC4E6\",\n    fill_kwargs={\"alpha\": 0.9},\n    ax=axes[\"C\"],\n    smooth=False,\n)\naxes[\"C\"].plot(\n    f_total_posterior.timestamp,\n    f_total_posterior.mean((\"chain\", \"draw\")),\n    color=\"#FBE64D\",\n    lw=2.5,\n)\naxes[\"C\"].set(xlabel=\"Timestamp\", ylabel=\"Nbr TDS\", title=\"Total GP\")\nplt.suptitle(\"Posterior GPs\", fontsize=18)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# this is just to please ggdist\n\nseasons_hsgp = read_parquet('writeup-dat/seasons-hsgp.parquet') |&gt;\n  rename(.chain = chain, .iteration = draw) |&gt;\n  mutate(.draw = row_number()) |&gt;\n  slice_sample(n = 50, by = .draw)\n\ngames_hsgp = read_parquet('writeup-dat/games-hsgp.parquet') |&gt;\n  rename(.chain = chain, .iteration = draw) |&gt;\n  mutate(.draw = row_number())\n\n\ntotal_hsgp = read_parquet('writeup-dat/total_hsgp.parquet') |&gt;\n  rename(.chain = chain, .iteration = draw)\n\nclrs = met.brewer(name = 'Lakota')\n\ng = ggplot(games_hsgp, aes(x = gameday, y = f_games)) +\n  geom_smooth(\n    aes(group = .iteration),\n    se = FALSE,\n    alpha = 0.3,\n    color = clrs[3],\n    linewidth = 0.5\n  ) +\n  stat_lineribbon(.width = (0.89), alpha = 0.4, fill = clrs[1]) +\n  labs(x = 'Weeks', y = 'Number of TDs')\n\n\ns = ggplot(seasons_hsgp, aes(x = seasons, y = f_season)) +\n  geom_smooth(\n    aes(group = .iteration),\n    se = FALSE,\n    alpha = 0.3,\n    color = clrs[3],\n    linewidth = 0.5\n  ) +\n  stat_lineribbon(.width = (0.89), alpha = 0.4, fill = clrs[1]) +\n  labs(x = 'Number of Seasons Played', y = 'Number of TDs')\n\nt = ggplot(total_hsgp, aes(x = timestamp, y = x)) +\n  geom_smooth(\n    aes(group = .iteration),\n    se = FALSE,\n    alpha = 0.3,\n    color = clrs[3],\n    linewidth = 0.5\n  ) +\n  stat_lineribbon(.width = (0.89), alpha = 0.4, fill = clrs[1]) +\n  labs(x = 'Timestamp', y = 'Number of TDs')\n\n(g + s) / t\n\n\n\n\n\n\n\n\n\n\n\n\nOverall, the model looks pretty good! The next avenues for exploration for modeling would be to nest players within their positions, so players are pulled closer to their position means rather than the mean of all pass catchers. I could see the argument either way that WRs are being penalized by RBs and TEs or the argument that RBs and TEs are being lifted by the WRs. Personally, I think that the estimates are probably biased downwards, where WR 2/3, TEs, and RBs are likely pulling better scoring threats downward rather than the other way around.\nThis has some utility if we think that we are underestimating a pass catcher’s true ability. Meaning that if our model predicts that they are better than league average, then we may be underestimating their ability by a little bit. This is nice for player evaluation because if we predict that a FB is a little bit worse of a pass catcher than we initially thought, then that’s not actually that big of a deal because we are using them as a battery ram. For a RB, than we are kind of just hoping they are a scoring threat as a runner and any additional production as a pass catcher is a nice to have.8\n\n\nPosterior Estimates\nNow to the fun part, plotting the data. When constructing the list of elite players, I tried to keep it simple by choosing the top 5 touchdown scorers for each position group. Then for the replacement level players, I just went with a host of players who scored 0 receiving touchdowns. I chose the 2023 season mostly because the 49ers’ offense was healthy and a juggernaut.\nLet’s first look at the season-level information about play above replacement. What is genuinely interesting to me is that, as a receiving scoring threat, Christian McCaffrey is mostly replaceable despite scoring 7 receiving touchdowns, which was a career high. Interestingly, we don’t have a ton of TEs that are super valuable despite a lot of LaPorta’s fantasy production coming from TDs in his rookie season. Interestingly, the RBs all have their Performance Above replacement, like right on the line. I think that this may just be telling us that as pass catchers, there probably isn’t a ton of additional value.\n\nPythonR\n\n\n\n\nCode\nmindex_coords = xr.Coordinates.from_pandas_multiindex(\n    cumulative_stats_pd.set_index(\n        [\n            \"receiver_full_name\",\n            \"number_of_seasons_played\",\n            \"games_played\",\n            \"season\",\n            \"receiver_position\"\n        ]\n    ).index,\n    \"obs_id\",\n)\nidata.posterior = idata.posterior.assign_coords(mindex_coords)\nidata.posterior_predictive = idata.posterior_predictive.assign_coords(mindex_coords)\n\n\nreplacement_list = (\n    cumulative_stats.unique([\"receiver_full_name\", \"season\"])\n    .with_columns(\n        pl.col(\"rec_tds_season\")\n        .rank(method=\"ordinal\")\n        .over([\"receiver_position\", \"season\"])\n        .alias(\"position_rank\")\n    )\n    .filter((pl.col(\"position_rank\") &lt;= 5) & (pl.col(\"season\") == 2023))[\n        \"receiver_full_name\"\n    ]\n)\n\nelite_list = (\n    cumulative_stats.unique([\"receiver_full_name\", \"season\"])\n    .with_columns(\n        pl.col(\"rec_tds_season\")\n        .rank(method=\"ordinal\", descending=True)\n        .over([\"receiver_position\", \"season\"])\n        .alias(\"position_rank\")\n    )\n    .filter((pl.col(\"position_rank\") &lt;= 5) & (pl.col(\"season\") == 2023))\n    .sort([\"receiver_position\", \"position_rank\"])[\"receiver_full_name\"]\n)\n\n\n\npost_preds = idata.posterior_predictive.reset_index('obs_id')\n\n#df = pl.from_dataframe(post_preds.to_dataframe())\n#\n#df.write_parquet('writeup-dat/posterior-predicitive.parquet')\n\nrpl_pef = post_preds[\"tds_scored\"].where(\n    (\n        (post_preds[\"receiver_full_name\"].isin(replacement_list))\n        & (post_preds[\"season\"] == 2023)\n    ),\n    drop=True,\n)\n\nelite_perf = post_preds[\"tds_scored\"].where(\n    (\n        (post_preds[\"receiver_full_name\"].isin(elite_list))\n        & (post_preds[\"season\"] == 2023)\n    ),\n    drop=True,\n)\n\n\n# Calculate PAR\nPAR = (\n    elite_perf.groupby([\"receiver_full_name\"]).mean(\"obs_id\") - rpl_pef.mean(\"obs_id\")\n).rename(\"PAR\")\n\n\nplayer_positions = (\n    cumulative_stats\n    .unique([\"receiver_full_name\", \"receiver_position\"])\n    .sort(\"receiver_position\")\n)\n\n\nposition_map = dict(zip(\n    player_positions[\"receiver_full_name\"],\n    player_positions[\"receiver_position\"]\n))\n\n\npar_df = PAR.to_dataframe().reset_index()\npar_df['position'] = par_df['receiver_full_name'].map(position_map)\n\n\npar_df = par_df.sort_values(['position', 'PAR'], ascending=[True, False])\n\n\nsorted_names = par_df['receiver_full_name'].tolist()\n\n\nPAR_sorted = PAR.sel(receiver_full_name=sorted_names)\n\n\naz.plot_forest(PAR_sorted, combined=True, colors=\"#6c1d0e\", figsize=(8, 12))\nax = plt.gca()\n\n\nlabs = [item.get_text() for item in ax.get_yticklabels()]\ncleaned_labs = []\nfor i in labs:\n    clean = i.replace(\"PAR[\", \"\").replace(\"]\", \"\").replace(\"[\", \"\")\n    cleaned_labs.append(clean)\n\nax.set_yticklabels(cleaned_labs)\n\n\nposition_counts = par_df.groupby('position').size()\ny_pos = 0\nfor pos in par_df['position'].unique():\n    count = position_counts[pos]\n    y_pos += count\n    if y_pos &lt; len(sorted_names):  \n        ax.axhline(y=y_pos - 0.5, color='gray', linestyle=':', alpha=0.3)\n\nax.axvline(c=\"k\", ls=\"--\", alpha=0.8)\nax.set(title=\"Performance Above Replacement\", xlabel=\"Receiving Touchdown\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplayers_stats = load_player_stats(seasons = c(2002:2024))\n\nelite = players_stats |&gt;\n  filter(position_group %in% c('RB', 'WR', 'TE')) |&gt;\n  mutate(\n    rec_tds_season = sum(receiving_tds),\n    .by = c(season, player_display_name)\n  ) |&gt;\n  group_by(season) |&gt;\n  distinct(player_display_name, .keep_all = TRUE) |&gt;\n  ungroup() |&gt;\n  mutate(\n    pos_rank = rank(-rec_tds_season, ties.method = 'first'),\n    .by = c(position_group, season)\n  ) |&gt;\n  filter(pos_rank %in% c(1:5), season == 2023) |&gt;\n  pull(player_display_name)\n\nreplacement = players_stats |&gt;\n  filter(position_group %in% c('RB', 'WR', 'TE')) |&gt;\n  mutate(\n    rec_tds_season = sum(receiving_tds),\n    .by = c(season, player_display_name)\n  ) |&gt;\n  group_by(season) |&gt;\n  distinct(player_display_name, .keep_all = TRUE) |&gt;\n  ungroup() |&gt;\n  mutate(\n    pos_rank = rank(rec_tds_season, ties.method = 'first'),\n    .by = c(position_group, season)\n  ) |&gt;\n  filter(pos_rank %in% c(1:5), season == 2023) |&gt;\n  pull(player_display_name)\n\n\nposterior_predictive = open_dataset('writeup-dat/posterior-predicitive.parquet')\n\nelite_df = posterior_predictive |&gt;\n  filter(receiver_full_name %in% elite, season == 2023) |&gt;\n  summarise(\n    rec_td_probs = mean(tds_scored),\n    .by = c(receiver_full_name, draw)\n  ) |&gt;\n  collect()\n\nreplacement_df = posterior_predictive |&gt;\n  filter(receiver_full_name %in% replacement, season == 2023) |&gt;\n  summarise(\n    rep_td_probs = mean(tds_scored),\n    .by = c(receiver_full_name, draw)\n  ) |&gt;\n  collect() |&gt;\n  select(-receiver_full_name)\n\npar_df = elite_df |&gt;\n  left_join(\n    replacement_df,\n    join_by(draw)\n  ) |&gt;\n  mutate(par = rec_td_probs - rep_td_probs) |&gt;\n  left_join(\n    players_stats |&gt;\n      filter(season == 2023) |&gt;\n      select(player_display_name, position_group),\n    join_by(receiver_full_name == player_display_name)\n  ) |&gt;\n  mutate(pos_num = case_match(position_group, 'RB' ~ 1, 'TE' ~ 2, 'WR' ~ 3))\n\n\nggplot(\n  par_df,\n  aes(x = par, y = fct_reorder(receiver_full_name, pos_num)),\n  fill = position_group\n) +\n  stat_halfeye() +\n  scale_fill_met_d(name = 'Lakota') +\n  geom_vline(xintercept = 0, linetype = 'dashed') +\n  labs(y = NULL, x = 'Play Above Replacement') +\n  theme(legend.position = 'none')\n\n\n\n\n\n\n\n\n\n\n\n\nWhat happens when we disregard the season-specific parameter? We do see some big jumps in value for almost everybody! This definitely makes more sense since Christian McCaffrey is generally regarded as a really good pass catcher, but especially at his position. Sam LaPorta has a bit more volatility than Hunter Henry and George Kittle, since, in the data, he is only a 2nd year player, so we just have less information about him.\n\nPythonR\n\n\n\n\nCode\nrpl_pef = post_preds[\"tds_scored\"].where(\n    (\n        (post_preds[\"receiver_full_name\"].isin(replacement_list))\n        #& (post_preds[\"season\"] == 2023)\n    ),\n    drop=True,\n)\n\nelite_perf = post_preds[\"tds_scored\"].where(\n    (\n        (post_preds[\"receiver_full_name\"].isin(elite_list))\n        #& (post_preds[\"season\"] == 2023)\n    ),\n    drop=True,\n)\n\n# Calculate PAR\nPAR = (\n    elite_perf.groupby([\"receiver_full_name\"]).mean(\"obs_id\") - rpl_pef.mean(\"obs_id\")\n).rename(\"PAR\")\n\n\nplayer_positions = (\n    cumulative_stats\n    .unique([\"receiver_full_name\", \"receiver_position\"])\n    .sort(\"receiver_position\")\n)\n\n\nposition_map = dict(zip(\n    player_positions[\"receiver_full_name\"],\n    player_positions[\"receiver_position\"]\n))\n\n\npar_df = PAR.to_dataframe().reset_index()\npar_df['position'] = par_df['receiver_full_name'].map(position_map)\n\n\npar_df = par_df.sort_values(['position', 'PAR'], ascending=[True, False])\n\n\nsorted_names = par_df['receiver_full_name'].tolist()\n\n\nPAR_sorted = PAR.sel(receiver_full_name=sorted_names)\n\n\naz.plot_forest(PAR_sorted, combined=True, colors=\"#6c1d0e\", figsize=(8, 12))\nax = plt.gca()\n\n\nlabs = [item.get_text() for item in ax.get_yticklabels()]\ncleaned_labs = []\nfor i in labs:\n    clean = i.replace(\"PAR[\", \"\").replace(\"]\", \"\").replace(\"[\", \"\")\n    cleaned_labs.append(clean)\n\nax.set_yticklabels(cleaned_labs)\n\n\n\nax.axvline(c=\"k\", ls=\"--\", alpha=0.8)\nax.set(title=\"Performance Above Replacement\", xlabel=\"Receiving Touchdown\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nelite_df = posterior_predictive |&gt;\n  filter(receiver_full_name %in% elite) |&gt;\n  summarise(\n    rec_td_probs = mean(tds_scored),\n    .by = c(receiver_full_name, draw)\n  ) |&gt;\n  collect()\n\nreplacement_df = posterior_predictive |&gt;\n  filter(receiver_full_name %in% replacement) |&gt;\n  summarise(\n    rep_td_probs = mean(tds_scored),\n    .by = c(receiver_full_name, draw)\n  ) |&gt;\n  collect() |&gt;\n  select(-receiver_full_name)\n\npar_df = elite_df |&gt;\n  left_join(\n    replacement_df,\n    join_by(draw)\n  ) |&gt;\n  mutate(par = rec_td_probs - rep_td_probs) |&gt;\n  left_join(\n    players_stats |&gt;\n      filter(season == 2023) |&gt;\n      select(player_display_name, position_group),\n    join_by(receiver_full_name == player_display_name)\n  ) |&gt;\n  mutate(pos_num = case_match(position_group, 'RB' ~ 1, 'TE' ~ 2, 'WR' ~ 3))\n\n\nggplot(\n  par_df,\n  aes(\n    x = par,\n    y = fct_reorder(receiver_full_name, pos_num),\n    fill = position_group\n  )\n) +\n  stat_halfeye() +\n  scale_fill_met_d(name = 'Lakota') +\n  geom_vline(xintercept = 0, linetype = 'dashed') +\n  labs(y = NULL, x = 'Play Above Replacement') +\n  theme(legend.position = 'none')\n\n\n\n\n\n\n\n\n\n\n\n\nPerhaps potentially one of the most interesting non-49ers comparison to make would be to look a the evolution of the Lion’s offense over Dan Campbell’s tenure as head coach. For a clean comparison, we are just going to use the 2022 Lions since Ben Johnson started his tenure as offensive coordinator that year. Over his tenure as offensive coordinator, the Lions went from a plucky team doing interesting things to one of the league’s best offenses.\nTo do this we are just going to take the posterior contrasts. I am particularly interested in two comparisons. The first is just how much better the offense is overall due to the maturation of Ben Johnson as a play caller and due to their investments into the offense. The Lions invested a fair amount of draft capital in the offense, 4 picks in the top 100 from 2021 to 2024. The second is looking at where the offense is getting more juice. My suspicion is just that part of the story is that they are just getting more juice from the RBs and maybe a touch more juice from the TE.\n\n\nCode\nplayers_2022 = cumulative_stats.filter(\n    (pl.col('posteam') == 'DET') & (pl.col('season') == 2022)\n).select(\n    pl.col('receiver_full_name').unique()\n)['receiver_full_name'].to_list()\n\nplayers_2024 = cumulative_stats.filter(\n    (pl.col('posteam') == 'DET') & (pl.col('season') == 2024)\n).select(\n    pl.col('receiver_full_name').unique()\n)['receiver_full_name'].to_list()\n\n\nperf_2022 = post_preds['tds_scored'].where(\n    (\n        (post_preds['receiver_full_name'].isin(players_2022))\n        & (post_preds['season'] == 2022)\n    ), drop = True\n)\n\nperf_2024 = post_preds['tds_scored'].where(\n    (\n        (post_preds['receiver_full_name'].isin(players_2024))\n        & (post_preds['season'] == 2024)\n    ), drop = True\n)\n\n\n\nleague_2022 = post_preds['tds_scored'].where(\n    (\n        (post_preds['season'] == 2022) &\n        (~post_preds['receiver_full_name'].isin(players_2022))\n    ), drop = True\n)\n\n\nleague_2024 = post_preds['tds_scored'].where(\n    (\n        (post_preds['season'] == 2024) &\n        (~post_preds['receiver_full_name'].isin(players_2024))\n    ), drop = True\n)\n\nteam_avg_2022 = perf_2022.mean('obs_id')\nteam_avg_2024 = perf_2024.mean('obs_id')\n\nleague_avg_2022 = league_2022.mean(\"obs_id\")\nleague_avg_2024 = league_2024.mean('obs_id')\n\n\nleague_pos_avg_2022 = league_2022.groupby([\"receiver_position\"]).mean('obs_id') \nleague_pos_avg_2024 = league_2022.groupby(['receiver_position']).mean('obs_id')\n\nlions_pos_avg_2022 = perf_2022.groupby([\"receiver_position\"]).mean('obs_id')\nlions_pos_avg_2024 = perf_2024.groupby(['receiver_position']).mean(\"obs_id\")\n\npos_contrast_2022 = (lions_pos_avg_2022 - league_pos_avg_2022).rename('PAR')\npos_contrast_2024 = (lions_pos_avg_2024 - league_pos_avg_2024).rename('PAR')\n\n\n\nLions_League_2022 = (team_avg_2022 - league_avg_2022).rename('Lions TDs Avg - League Avg')\nLions_League_2024 = (team_avg_2024 - league_avg_2024).rename('Lions TDs Avg - League Avg')\n\n\n\n_,(right, left) = plt.subplots(1,2)\n\n\naz.plot_forest([Lions_League_2022, Lions_League_2024],\n                model_names = ['Lions v League 2022', 'Lions v League 2024'],\n                combined = True,\n                ax = right)\naz.plot_forest([pos_contrast_2022, pos_contrast_2024],\n                model_names = ['Lions v League 2022', 'Lions v League 2024'],\n                combined = True,\n                ax = left\n                )\n\nright.set(\n    title = 'Difference in Performance Between \\n 2022 Lions and 2024 Lions'\n)\nleft.set(\n    title = 'Performance by Position'\n)\nright.axvline(c = 'k', ls = '--', alpha = 0.4)\nleft.axvline(c = 'k', ls = '--', alpha = 0.4)\nleft.get_legend().remove()\n\n\n\n\n\n\n\n\n\nIt is clear that the Lion’s offense got better from Ben Johnson’s first year as a play caller to his last year as a play caller in Detroit. When we break down the differences by position, we see some interesting trends. Both the WRs and TEs improve between 2022 and 2024 if only slightly. Interestingly, the RB position is kind of the same. This is a bit puzzling, as Jahmyr Gibbs is a pretty good pass catcher as a running back.\nI guess that the part of the reason we see improvements for the pass catchers is not only due to maturation of Amon-Ra St. Brown and upgrades at WR and TE.9 Part of this improvement is likely due to how well the Lions run the ball. Part of their success running the ball is that their pass catchers are good to excellent blockers, and their O-Line is good. What this means is that they are able to run and pass the ball out of the same formations. This puts a lot of stress on a defense as Linebackers and Safeties are forced to either come forward to defend the run or the defense puts bigger bodies on the field to defend the run. This opens up opportunities in the passing games to hit explosive plays or take additional defenders in the red zone, so other pass catchers can get open.\nIf we want to see the evolution of a pass catcher’s ability, we can just plot the probabilities over the number of games they played for each season. What we see is that the probability that Amon-Ra scores a touchdown or doesn’t score a touchdown starts to be in the same neighborhood. Which is crazy! This speaks to not only his maturation as a pass catcher, but also the maturation of the system that he is in.\n\nPythonR\n\n\n\n\nCode\n\n# this looks pretty gross so I am just going to do this in ggplot\nplayer = \"Amon-Ra St. Brown\"\nidata.posterior = idata.posterior.rename({\"tds_scored_probs_dim_1\": \"event\"})\n\n\nimplied_probs = (\n    idata.posterior[\"tds_scored_probs\"]\n    .rename({\"tds_scored_probs_dim_0\": \"obs_id\"})\n    .assign_coords(mindex_coords)\n)\n\n\nplayer_probs_post = implied_probs.sel(receiver_full_name=player)\ncolors = plt.cm.viridis(np.linspace(0.05, 0.95, 4))\n\ncols = 2\nunique_seasons = np.unique(player_probs_post.season)\nnum_seasons = len(unique_seasons)\nrows = (num_seasons + cols - 1) // cols\n\nfig, axes = plt.subplots(\n    rows, cols, figsize=(12, 2.5 * rows), layout=\"constrained\", sharey=True\n)\n\naxes = axes.flatten()\n\n\nplayer_probs_post_df = player_probs_post.to_dataframe()\n\nfor season, (i, ax) in zip(unique_seasons, enumerate(axes)):\n    dates = player_probs_post.sel(season=season)[\"games_played\"]\n    y_plot = player_probs_post.sel(season=season)\n\n    for event in player_probs_post.event.to_numpy():\n        az.plot_hdi(\n            x=dates,\n            y=y_plot.sel(event=event),\n            hdi_prob=0.89,\n            color=colors[event],\n            fill_kwargs={\"alpha\": 0.4},\n            ax=ax,\n            smooth=False,\n        )\n        ax.plot(\n            dates,\n            y_plot.sel(event=event).mean((\"chain\", \"draw\")),\n            lw=2,\n            ls=\"--\",\n            label=f\"{event}\",\n            color=colors[event],\n            alpha=0.9,\n        )\n\n    ax.set(xlabel=\"Day\", ylabel=\"Probability\", title=f\"{season}\")\n    sns.despine()\n    if i == 0:\n        ax.legend(fontsize=10, frameon=True, title=\"TDs\", ncols=4)\n\n# remove any unused subplots\nfor j in range(i + 1, len(axes)):\n    fig.delaxes(axes[j])\n\nplt.suptitle(f\"{player.title()}\\nTDs Probabilities per Season\", fontsize=18)\n\n\n# df = pl.from_dataframe(implied_probs.to_dataframe())\n#\n#\n# df.write_parquet('writeup-dat/implied_probs.parquet')\n# \n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimplied_probs_data = open_dataset('writeup-dat/implied_probs.parquet') |&gt;\n  rename(\n    #n_tds = tds_scored_probs_dim_1,\n    .chain = chain,\n    .iteration = draw\n  ) |&gt;\n  select(-(contains('index_level')))\n\namon_ra = implied_probs_data |&gt;\n  filter(receiver_full_name == 'Amon-Ra St. Brown') |&gt;\n  collect() |&gt;\n  mutate(.draw = row_number(), n_tds = as.factor(event))\n\n\nggplot(amon_ra, aes(x = games_played, y = tds_scored_probs, fill = n_tds)) +\n  stat_lineribbon(aes(fill_ramp = after_stat(level))) +\n  scale_fill_met_d(name = 'Troy') +\n  facet_wrap(vars(season)) +\n  labs(\n    y = 'TD Scoring Probabilities',\n    fill = 'Number of TDs',\n    x = 'Week',\n    fill_ramp = 'Level'\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nOne of the downsides the nflreadr data is that we do not have prime Jerry Rice in there. They do have his last 2 years as a Niner and his time as a Raider and Seahawk in there, but he was in years 14 to 19. Compared to prime Jerry Rice he had declined significantly, with his last double-digit touchdown season coming during the 1995 season. So instead, we will just look at some of the most dominant pass catchers in NFL from 2002-2024 and some of their younger contemporaries.\nPlayers kind towards the end of their careers and at the beginning get short changed a bit but it is pretty to cool to see just how good some of these guys are. It is crazy to think that for some players that by the act of showing up they are pretty likely to score a touchdown! What is interesting is that for good pass-catching backs generally just increase their probability to about 25%. Which seems small but you are getting a touch more juice from a fantasy RB spot if they catch a touchdown and rush for a touchdown.\n\n\nCode\nactive_player = c(\n  'Travis Kelce',\n  'George Kittle', \n  'Davante Adams',\n  'Mike Evans',\n  'Justin Jefferson', \n  \"Ja'Marr Chase\",\n  'Christian McCaffrey',\n  'Austin Ekeler',\n  'Alvin Kamara',\n  'Saquon Barkley'\n)\n\n\nall_timers = c(\n  'Jimmy Graham',\n  'Rob Gronkowski',\n  'Tony Gonzalez',\n  'Jason Witten',\n  'Terrel Owens', \n  'Antonio Gates',\n  'Randy Moss',\n  'Larry Fitzgerald', \n  'Julio Jones',\n  'Calvin Johnson', \n  \"Marshall Faulk\"\n  )\nplayers_interested_in = c(all_timers, active_player)\n\n\njoin_these = players_stats |&gt;\n  filter(player_display_name %in% players_interested_in, position_group %in% c('RB', 'WR', 'TE')) |&gt;\n  distinct(player_display_name, .keep_all = TRUE) |&gt;\n  select(receiver_full_name = player_display_name, position_group)\n\nplot_data = implied_probs_data |&gt;\n  filter(receiver_full_name %in% players_interested_in) |&gt;\n  collect() |&gt;\n  left_join(join_these) |&gt;\n  mutate(\n    n_tds = as.factor(event),\n    pos_num = case_match(position_group, 'RB' ~ 1, 'TE' ~ 2, 'WR' ~ 3),\n    active = ifelse(receiver_full_name %in% active_player, 'Active Player', 'Retired Player')\n  )\n\nactive_p = ggplot(\n  data = filter(plot_data, active == 'Active Player'),\n  aes(\n      x = tds_scored_probs,\n      y = fct_reorder(receiver_full_name, pos_num),\n      color = n_tds\n   )\n) +\n    stat_pointinterval(linewidth = 0.5, size = 0.5) +\n    scale_color_met_d(name = 'Lakota') +\n    labs(x = 'TD Scoring Probabilities',\n        title = 'Active Players',\n        y = NULL,\n        color = 'Number of TDs')\n\nretired_p = ggplot(\n  data = filter(plot_data, active == 'Retired Player'),\n  aes(\n      x = tds_scored_probs,\n      y = fct_reorder(receiver_full_name, pos_num),\n      color = n_tds\n)\n) +\n    stat_pointinterval(linewidth = 0.5, size = 0.5) +\n    scale_color_met_d(name = 'Lakota') +\n    labs(x = 'TD Scoring Probabilities',\n        y = NULL, color = 'Number of TDs',\n        title = 'Retired Players')\n\nactive_p / retired_p + plot_layout(axis_titles = 'collect', guides = 'collect')\n\n\n\n\n\n\n\n\n\nIf there are a group of players that you are interested feel free to play around with the data in the web-r code block!\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "blog/2025/passing-td-factor-model/index.html#footnotes",
    "href": "blog/2025/passing-td-factor-model/index.html#footnotes",
    "title": "Modeling Receiver Ability Bayesiansly",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs a disclaimer, I am a huge 49ers fan, so a lot of examples will be 49ers-centric.↩︎\nFor fun, I just try to do things in both matplotlib and ggplot2. I personally prefer working with posteriors in ggplot2 because ggdist has a lot of great uncertainty features and produces less busy plots.↩︎\nI am American, so I will just call it Soccer and call American Football Football.↩︎\nTechnically, the play-by-play data goes from 1999-2024 but some of the data that are used for building the model are missing. Therefore I elect to use the 2002-2024 seasons.↩︎\nTaylor Swift lyric intended.↩︎\nI am currently toying with using an Ordered Beta to see if this does a better job.↩︎\nThanks to the from_dataframe feature in polars getting the data to plot in ggplot was a cinch.↩︎\nIt would be interesting to see if there is a viable RDD for RBs on screens since the pass air yards on the pass is probably clustered in and around -1 to 1 air yards.↩︎\nThis is not a slight to T.J. Hockenson, who is an excellent TE.↩︎"
  },
  {
    "objectID": "blog/2024/translating-dplyr-to-polars/index.html",
    "href": "blog/2024/translating-dplyr-to-polars/index.html",
    "title": "Translating What I know in the tidyverse to polars:",
    "section": "",
    "text": "I suppose at some point it is good to become more well versed in lots of tools. I have been python curious for about a year or so and I think it is important to use the tool best suited for the task. Also sometimes it is important to get out of your comfort zone. I am definitely somebody who is very comfortable in R and the tidyverse and use it for a lot of stuff. I have heard lots of ravings about polars specifically about its speed and similarities in intuition with the tidyverse. So I thought I would have a collection of code for myself and the people of the internet to reference.\nJust a disclaimer. This is really just me working through the similarities and is going to be based on the tidyintelligence’s blog post, Robert Mitchell’s blog post, and Emily Rieder’s blog post. In all honesty, this is just for me to smash them together to have a one-stop shop for myself. If you found this post over these resources I highly recommend you check out these resources."
  },
  {
    "objectID": "blog/2024/translating-dplyr-to-polars/index.html#r-6",
    "href": "blog/2024/translating-dplyr-to-polars/index.html#r-6",
    "title": "Translating What I know in the tidyverse to polars:",
    "section": "R",
    "text": "R\n\npenguins |&gt;\nfilter(!species %in% c(\"Gentoo\", \"Chinstrap\"),\n       island != \"Dream\")\n\n# A tibble: 96 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 86 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "blog/2024/translating-dplyr-to-polars/index.html#python-6",
    "href": "blog/2024/translating-dplyr-to-polars/index.html#python-6",
    "title": "Translating What I know in the tidyverse to polars:",
    "section": "Python",
    "text": "Python\n\n# | error: true\n# | label: set-filter-not\n\npenguins.filter((pl.col(\"species\").is_in([\"Chinstrap\", \"Gentoo\"]).not_()) &\n                (pl.col(\"island\") != 'Dream'))\n\n\nshape: (96, 8)\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\nstr\nstr\nf64\nf64\nf64\nf64\nstr\ni64\n\n\n\n\n\"Adelie\"\n\"Torgersen\"\n39.1\n18.7\n181.0\n3750.0\n\"male\"\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n39.5\n17.4\n186.0\n3800.0\n\"female\"\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n40.3\n18.0\n195.0\n3250.0\n\"female\"\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\nnull\nnull\nnull\nnull\nnull\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n36.7\n19.3\n193.0\n3450.0\n\"female\"\n2007\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Adelie\"\n\"Torgersen\"\n41.5\n18.3\n195.0\n4300.0\n\"male\"\n2009\n\n\n\"Adelie\"\n\"Torgersen\"\n39.0\n17.1\n191.0\n3050.0\n\"female\"\n2009\n\n\n\"Adelie\"\n\"Torgersen\"\n44.1\n18.0\n210.0\n4000.0\n\"male\"\n2009\n\n\n\"Adelie\"\n\"Torgersen\"\n38.5\n17.9\n190.0\n3325.0\n\"female\"\n2009\n\n\n\"Adelie\"\n\"Torgersen\"\n43.1\n19.2\n197.0\n3500.0\n\"male\"\n2009\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "blog/2024/translating-dplyr-to-polars/index.html#group-by-and-summarize",
    "href": "blog/2024/translating-dplyr-to-polars/index.html#group-by-and-summarize",
    "title": "Translating What I know in the tidyverse to polars:",
    "section": "Group by and summarize",
    "text": "Group by and summarize\nLast but not least we need to do the group by and summarise bit. It looks like this is slightly more intuitive\n\nRPython\n\n\n\npenguins |&gt;\ngroup_by(species) |&gt;\nsummarise(total = n())\n\n# A tibble: 3 × 2\n  species   total\n  &lt;fct&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\n\n\n\npenguins.group_by(pl.col(\"species\")).agg(total = pl.count())\n\n\nshape: (3, 2)\n\n\n\nspecies\ntotal\n\n\nstr\nu32\n\n\n\n\n\"Adelie\"\n152\n\n\n\"Gentoo\"\n124\n\n\n\"Chinstrap\"\n68\n\n\n\n\n\n\n\n\n\nLets do some mathy stuff\n\npenguins.group_by(pl.col(\"species\")).agg(count = pl.len(),\n                                         mean_flipp = pl.mean(\"flipper_length_mm\"),\n                                         median_flipp = pl.median(\"flipper_length_mm\"))\n\n\nshape: (3, 4)\n\n\n\nspecies\ncount\nmean_flipp\nmedian_flipp\n\n\nstr\nu32\nf64\nf64\n\n\n\n\n\"Adelie\"\n152\n189.953642\n190.0\n\n\n\"Chinstrap\"\n68\n195.823529\n196.0\n\n\n\"Gentoo\"\n124\n217.186992\n216.0\n\n\n\n\n\n\n\nacross\nA thing that is useful in summarize is that we can use our selectors to summarise across multiple columns like this\n\npenguins |&gt;\ngroup_by(species) |&gt;\nsummarise(across(starts_with(\"bill\"), list(mean = \\(x) mean(x, na.rm = TRUE,\n                                           median = \\(x) median(x, na.rm,  TRUE)))))\n\n# A tibble: 3 × 3\n  species   bill_length_mm_mean bill_depth_mm_mean\n  &lt;fct&gt;                   &lt;dbl&gt;              &lt;dbl&gt;\n1 Adelie                   38.8               18.3\n2 Chinstrap                48.8               18.4\n3 Gentoo                   47.5               15.0\n\n\nIn polars I imagine it would probably be something like this\n\npenguins.group_by(pl.col(\"species\")).agg(cs.starts_with(\"bill\").mean())\n\n\nshape: (3, 3)\n\n\n\nspecies\nbill_length_mm\nbill_depth_mm\n\n\nstr\nf64\nf64\n\n\n\n\n\"Chinstrap\"\n48.833824\n18.420588\n\n\n\"Adelie\"\n38.791391\n18.346358\n\n\n\"Gentoo\"\n47.504878\n14.982114\n\n\n\n\n\n\nThe think I am running into now is that I would like to add a _ without doing any extra work. It looks like according to the docs it should be this\n\npenguins.group_by(pl.col(\"species\")).agg(cs.starts_with(\"bill\").mean().name.suffix(\"_mean\"),\n                                         cs.starts_with(\"bill\").median().name.suffix(\"_median\"))\n\n\nshape: (3, 5)\n\n\n\nspecies\nbill_length_mm_mean\nbill_depth_mm_mean\nbill_length_mm_median\nbill_depth_mm_median\n\n\nstr\nf64\nf64\nf64\nf64\n\n\n\n\n\"Chinstrap\"\n48.833824\n18.420588\n49.55\n18.45\n\n\n\"Gentoo\"\n47.504878\n14.982114\n47.3\n15.0\n\n\n\"Adelie\"\n38.791391\n18.346358\n38.8\n18.4"
  },
  {
    "objectID": "blog/2024/translating-dplyr-to-polars/index.html#pivots-of-all-shapes",
    "href": "blog/2024/translating-dplyr-to-polars/index.html#pivots-of-all-shapes",
    "title": "Translating What I know in the tidyverse to polars:",
    "section": "Pivots of all shapes",
    "text": "Pivots of all shapes\nSometimes we need to pivot our data. Lets use the built in example from tidyr. Basically we have a whole bunch of columns that denote counts of income brackets\n\nrelig = relig_income\n\n\nwrite_csv(relig,\"relig_income.csv\")\n\n\nhead(relig_income)\n\n# A tibble: 6 × 11\n  religion  `&lt;$10k` `$10-20k` `$20-30k` `$30-40k` `$40-50k` `$50-75k` `$75-100k`\n  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 Agnostic       27        34        60        81        76       137        122\n2 Atheist        12        27        37        52        35        70         73\n3 Buddhist       27        21        30        34        33        58         62\n4 Catholic      418       617       732       670       638      1116        949\n5 Don’t kn…      15        14        15        11        10        35         21\n6 Evangeli…     575       869      1064       982       881      1486        949\n# ℹ 3 more variables: `$100-150k` &lt;dbl&gt;, `&gt;150k` &lt;dbl&gt;,\n#   `Don't know/refused` &lt;dbl&gt;\n\n\nIn tidyr we would just do this\n\nrelig |&gt;\npivot_longer(-religion,\n              names_to = \"income_bracket\",\n              values_to = \"count\")\n\n# A tibble: 180 × 3\n   religion income_bracket     count\n   &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;\n 1 Agnostic &lt;$10k                 27\n 2 Agnostic $10-20k               34\n 3 Agnostic $20-30k               60\n 4 Agnostic $30-40k               81\n 5 Agnostic $40-50k               76\n 6 Agnostic $50-75k              137\n 7 Agnostic $75-100k             122\n 8 Agnostic $100-150k            109\n 9 Agnostic &gt;150k                 84\n10 Agnostic Don't know/refused    96\n# ℹ 170 more rows\n\n\nwhich is nice because we can just identify a column and then pivot. One thing that I will have to just memorize is that when we are moving things to long in polars than we melt the dataframe. Kind of like a popsicle or something. The mnemonic device will come to me eventually\n\nrelig = pl.read_csv(\"relig_income.csv\")\n\nrelig.head()\n\n\nshape: (5, 11)\n\n\n\nreligion\n&lt;$10k\n$10-20k\n$20-30k\n$30-40k\n$40-50k\n$50-75k\n$75-100k\n$100-150k\n&gt;150k\nDon't know/refused\n\n\nstr\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\n\n\n\n\n\"Agnostic\"\n27\n34\n60\n81\n76\n137\n122\n109\n84\n96\n\n\n\"Atheist\"\n12\n27\n37\n52\n35\n70\n73\n59\n74\n76\n\n\n\"Buddhist\"\n27\n21\n30\n34\n33\n58\n62\n39\n53\n54\n\n\n\"Catholic\"\n418\n617\n732\n670\n638\n1116\n949\n792\n633\n1489\n\n\n\"Don’t know/refused\"\n15\n14\n15\n11\n10\n35\n21\n17\n18\n116\n\n\n\n\n\n\nTo melt all we do is\n\nrelig.melt(id_vars = \"religion\", variable_name = \"income_bracket\", value_name = \"count\")\n\n\nshape: (180, 3)\n\n\n\nreligion\nincome_bracket\ncount\n\n\nstr\nstr\ni64\n\n\n\n\n\"Agnostic\"\n\"&lt;$10k\"\n27\n\n\n\"Atheist\"\n\"&lt;$10k\"\n12\n\n\n\"Buddhist\"\n\"&lt;$10k\"\n27\n\n\n\"Catholic\"\n\"&lt;$10k\"\n418\n\n\n\"Don’t know/refused\"\n\"&lt;$10k\"\n15\n\n\n…\n…\n…\n\n\n\"Orthodox\"\n\"Don't know/refused\"\n73\n\n\n\"Other Christian\"\n\"Don't know/refused\"\n18\n\n\n\"Other Faiths\"\n\"Don't know/refused\"\n71\n\n\n\"Other World Religions\"\n\"Don't know/refused\"\n8\n\n\n\"Unaffiliated\"\n\"Don't know/refused\"\n597\n\n\n\n\n\n\nsame would go for the pivoting wider\n\npenguins.pivot(index = \"island\",columns = \"species\", values = \"body_mass_g\",\n              aggregate_function=\"sum\")\n\n\nshape: (3, 4)\n\n\n\nisland\nAdelie\nGentoo\nChinstrap\n\n\nstr\nf64\nf64\nf64\n\n\n\n\n\"Torgersen\"\n189025.0\nnull\nnull\n\n\n\"Biscoe\"\n163225.0\n624350.0\nnull\n\n\n\"Dream\"\n206550.0\nnull\n253850.0\n\n\n\n\n\n\nthis isn’t quite the same because we are aggregating it. This is likely just a skill issue on the user end. But still we have wide data now!\n\nUsing selectors in pivot longer\nA slightly more complex example is using the billboards datas\n\nbillboards = tidyr::billboard\n\n\nwrite_csv(billboards, \"billboard.csv\")\n\n\nhead(billboards)\n\n# A tibble: 6 × 79\n  artist      track date.entered   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8\n  &lt;chr&gt;       &lt;chr&gt; &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 2 Pac       Baby… 2000-02-26      87    82    72    77    87    94    99    NA\n2 2Ge+her     The … 2000-09-02      91    87    92    NA    NA    NA    NA    NA\n3 3 Doors Do… Kryp… 2000-04-08      81    70    68    67    66    57    54    53\n4 3 Doors Do… Loser 2000-10-21      76    76    72    69    67    65    55    59\n5 504 Boyz    Wobb… 2000-04-15      57    34    25    17    17    31    36    49\n6 98^0        Give… 2000-08-19      51    39    34    26    26    19     2     2\n# ℹ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;,\n#   wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;,\n#   wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;,\n#   wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;,\n#   wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;,\n#   wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;,\n#   wk43 &lt;dbl&gt;, wk44 &lt;dbl&gt;, wk45 &lt;dbl&gt;, wk46 &lt;dbl&gt;, wk47 &lt;dbl&gt;, wk48 &lt;dbl&gt;, …\n\n billboards |&gt;\npivot_longer(cols = starts_with(\"wk\"),\n              names_to = \"week\",\n              values_to = \"count_of_weeks\")\n\n# A tibble: 24,092 × 5\n   artist track                   date.entered week  count_of_weeks\n   &lt;chr&gt;  &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt;          &lt;dbl&gt;\n 1 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk1               87\n 2 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk2               82\n 3 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk3               72\n 4 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk4               77\n 5 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk5               87\n 6 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk6               94\n 7 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk7               99\n 8 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk8               NA\n 9 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk9               NA\n10 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk10              NA\n# ℹ 24,082 more rows\n\n\nWe can do something similar with polars by using our selectors.\n\nbillboards = pl.read_csv(\"billboard.csv\")\n\n\nbillboards.melt(id_vars = \"artist\",value_vars  = cs.starts_with(\"wk\"),\n                variable_name = \"week\", value_name = \"count\" )\n\n\nshape: (24_092, 3)\n\n\n\nartist\nweek\ncount\n\n\nstr\nstr\nstr\n\n\n\n\n\"2 Pac\"\n\"wk1\"\n\"87\"\n\n\n\"2Ge+her\"\n\"wk1\"\n\"91\"\n\n\n\"3 Doors Down\"\n\"wk1\"\n\"81\"\n\n\n\"3 Doors Down\"\n\"wk1\"\n\"76\"\n\n\n\"504 Boyz\"\n\"wk1\"\n\"57\"\n\n\n…\n…\n…\n\n\n\"Yankee Grey\"\n\"wk76\"\n\"NA\"\n\n\n\"Yearwood, Trisha\"\n\"wk76\"\n\"NA\"\n\n\n\"Ying Yang Twins\"\n\"wk76\"\n\"NA\"\n\n\n\"Zombie Nation\"\n\"wk76\"\n\"NA\"\n\n\n\"matchbox twenty\"\n\"wk76\"\n\"NA\"\n\n\n\n\n\n\nBroadly it works the same but if you don’t specify the id vars you will end up with just the week and count column"
  },
  {
    "objectID": "blog/2024/translating-dplyr-to-polars/index.html#unnest",
    "href": "blog/2024/translating-dplyr-to-polars/index.html#unnest",
    "title": "Translating What I know in the tidyverse to polars:",
    "section": "Unnest",
    "text": "Unnest\nSometimes we have these unfriendly list columns that we would like to make not lists. Lets go ahead and use the starwars list columns.\n\nstarwars_lists = starwars |&gt;\nselect(name, where(is.list)) |&gt;\nunnest_longer(starships , keep_empty = TRUE) |&gt;\nunnest_longer(films, keep_empty = TRUE) |&gt;\nunnest_longer(vehicles, keep_empty = TRUE)\n\n\n\n\n\nhead(starwars_lists)\n\n# A tibble: 6 × 4\n  name           films                   vehicles              starships\n  &lt;chr&gt;          &lt;chr&gt;                   &lt;chr&gt;                 &lt;chr&gt;    \n1 Luke Skywalker A New Hope              Snowspeeder           X-wing   \n2 Luke Skywalker A New Hope              Imperial Speeder Bike X-wing   \n3 Luke Skywalker The Empire Strikes Back Snowspeeder           X-wing   \n4 Luke Skywalker The Empire Strikes Back Imperial Speeder Bike X-wing   \n5 Luke Skywalker Return of the Jedi      Snowspeeder           X-wing   \n6 Luke Skywalker Return of the Jedi      Imperial Speeder Bike X-wing   \n\n\nIn polars we have a similarish function named explode. Unfortunately we don’t have a a selector for all attribute types so we are going to do this by hand.\n\nstarwars_list = starwars.select([\"name\", \"films\", \"vehicles\", \"starships\"])\n\nstarwars_list.glimpse()\n\nRows: 87\nColumns: 4\n$ name            &lt;str&gt; 'Luke Skywalker', 'C-3PO', 'R2-D2', 'Darth Vader', 'Leia Organa', 'Owen Lars', 'Beru Whitesun Lars', 'R5-D4', 'Biggs Darklighter', 'Obi-Wan Kenobi'\n$ films     &lt;list[str]&gt; ['A New Hope', 'The Empire Strikes Back', 'Return of the Jedi', 'Revenge of the Sith', 'The Force Awakens'], ['A New Hope', 'The Empire Strikes Back', 'Return of the Jedi', 'The Phantom Menace', 'Attack of the Clones', 'Revenge of the Sith'], ['A New Hope', 'The Empire Strikes Back', 'Return of the Jedi', 'The Phantom Menace', 'Attack of the Clones', 'Revenge of the Sith', 'The Force Awakens'], ['A New Hope', 'The Empire Strikes Back', 'Return of the Jedi', 'Revenge of the Sith'], ['A New Hope', 'The Empire Strikes Back', 'Return of the Jedi', 'Revenge of the Sith', 'The Force Awakens'], ['A New Hope', 'Attack of the Clones', 'Revenge of the Sith'], ['A New Hope', 'Attack of the Clones', 'Revenge of the Sith'], ['A New Hope'], ['A New Hope'], ['A New Hope', 'The Empire Strikes Back', 'Return of the Jedi', 'The Phantom Menace', 'Attack of the Clones', 'Revenge of the Sith']\n$ vehicles  &lt;list[str]&gt; ['Snowspeeder', 'Imperial Speeder Bike'], [], [], [], ['Imperial Speeder Bike'], [], [], [], [], ['Tribubble bongo']\n$ starships &lt;list[str]&gt; ['X-wing', 'Imperial shuttle'], [], [], ['TIE Advanced x1'], [], [], [], [], ['X-wing'], ['Jedi starfighter', 'Trade Federation cruiser', 'Naboo star skiff', 'Jedi Interceptor', 'Belbullab-22 starfighter']\n\nstarwars_explode =  starwars_list.explode(\"films\").explode(\"vehicles\").explode(\"starships\")\n\nstarwars_explode.head()\n\n\nshape: (5, 4)\n\n\n\nname\nfilms\nvehicles\nstarships\n\n\nstr\nstr\nstr\nstr\n\n\n\n\n\"Luke Skywalker\"\n\"A New Hope\"\n\"Snowspeeder\"\n\"X-wing\"\n\n\n\"Luke Skywalker\"\n\"A New Hope\"\n\"Snowspeeder\"\n\"Imperial shuttle\"\n\n\n\"Luke Skywalker\"\n\"A New Hope\"\n\"Imperial Speeder Bike\"\n\"X-wing\"\n\n\n\"Luke Skywalker\"\n\"A New Hope\"\n\"Imperial Speeder Bike\"\n\"Imperial shuttle\"\n\n\n\"Luke Skywalker\"\n\"The Empire Strikes Back\"\n\"Snowspeeder\"\n\"X-wing\""
  },
  {
    "objectID": "blog/2024/translating-dplyr-to-polars/index.html#histograms",
    "href": "blog/2024/translating-dplyr-to-polars/index.html#histograms",
    "title": "Translating What I know in the tidyverse to polars:",
    "section": "Histograms",
    "text": "Histograms\n\n\nsns.histplot(data = penguins, x = \"body_mass_g\")\n\n\n\n\n\n\n\n\neasy enough\n\n\nsns.histplot(data = penguins, x = \"body_mass_g\", hue  = \"species\")\n\n\n\n\n\n\n\n\nWhat if we wanted densities instead of frequencies? In ggplot it would be\n\nggplot(penguins, aes(x = body_mass_g, fill = species)) +\ngeom_histogram(aes(y =  after_stat(density)))\n\n\n\n\n\n\n\n\nIn sns it would be.\n\n\nsns.histplot(data = penguins, x = \"body_mass_g\", hue = \"species\", stat = \"density\")\n\n\n\n\n\n\n\n\nI really like the legend on the inside! In new ggplot 3.whatever it is different now that there is an explicit legend.position=\"inside\".\n\nggplot(penguins, aes(x = body_mass_g, y = after_stat(density), fill = species)) +\ngeom_histogram() +\ntheme_minimal() +\ntheme(legend.position = c(.95,.95),\n      legend.justification = c(\"right\", \"top\"))\n\n\n\n\n\n\n\n\nCool I like that and will fiddle with that my ggplot theme!\nOkay lets now go and do some bivariate plots. Obviously the workhorse of bivariate plotting is the scatterplot ."
  },
  {
    "objectID": "blog/2024/translating-dplyr-to-polars/index.html#scatter-plots",
    "href": "blog/2024/translating-dplyr-to-polars/index.html#scatter-plots",
    "title": "Translating What I know in the tidyverse to polars:",
    "section": "Scatter Plots",
    "text": "Scatter Plots\n\n\nsns.scatterplot(data = penguins, x = \"flipper_length_mm\", y = \"body_mass_g\", hue = \"species\")\n\n\n\n\n\n\n\n\nthe next thing that we would want is to size points by the size of the penguin\n\n\nsns.scatterplot(data = penguins, x = \"flipper_length_mm\", y = \"body_mass_g\", hue = \"species\", size = \"body_mass_g\")\n\n\n\n\n\n\n\n\nThat is not really great since the legend is inside and covering stuff. In ggplot we would simply just move the legend position. In this case we have to save it as an object"
  },
  {
    "objectID": "blog/2024/translating-dplyr-to-polars/index.html#adjusting-legend",
    "href": "blog/2024/translating-dplyr-to-polars/index.html#adjusting-legend",
    "title": "Translating What I know in the tidyverse to polars:",
    "section": "Adjusting legend",
    "text": "Adjusting legend\n\n\nexmp = sns.scatterplot(data = penguins, x = \"flipper_length_mm\", y = \"body_mass_g\", hue = \"species\", size = \"body_mass_g\")\n\nsns.move_legend(exmp, \"upper left\", bbox_to_anchor = (1,1))\n\n\n\n\n\n\n\n\n\nsns.lmplot(data = penguins, x = \"flipper_length_mm\", y = \"body_mass_g\", hue = \"species\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThen the other one that I use all the time is using a line of best fit . Okay obviously the most annoying part is that we don’t have great labels"
  },
  {
    "objectID": "blog/2024/translating-dplyr-to-polars/index.html#adding-informative-labels",
    "href": "blog/2024/translating-dplyr-to-polars/index.html#adding-informative-labels",
    "title": "Translating What I know in the tidyverse to polars:",
    "section": "Adding Informative Labels",
    "text": "Adding Informative Labels\n\nlabs_examp = sns.lmplot(data = penguins, x = \"flipper_length_mm\", y = \"body_mass_g\", hue = \"species\")\n\n\n\nlabs_examp.set_axis_labels(x_var= \"Flipper Length(mm)\", y_var = \"Body Mass(g)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne of the things we may want to do is to create small multiples."
  },
  {
    "objectID": "blog/2024/translating-dplyr-to-polars/index.html#facet-wrap",
    "href": "blog/2024/translating-dplyr-to-polars/index.html#facet-wrap",
    "title": "Translating What I know in the tidyverse to polars:",
    "section": "Facet Wrap",
    "text": "Facet Wrap\n\nsns.displot(data = penguins, x = \"body_mass_g\", hue = \"species\", row= \"species\", facet_kws = dict(margin_titles=False))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI am honestly not wild about the plot but that is life"
  },
  {
    "objectID": "blog/2024/translating-dplyr-to-polars/index.html#make-a-column-into-a-vector",
    "href": "blog/2024/translating-dplyr-to-polars/index.html#make-a-column-into-a-vector",
    "title": "Translating What I know in the tidyverse to polars:",
    "section": "Make a column into a vector",
    "text": "Make a column into a vector\nIn R there are like a ton of different ways to do this\n\nvec1 = penguins$bill_depth_mm\n\nvec2 = penguins |&gt;\npluck(\"bill_depth_mm\")\n\nvec3 = penguins |&gt;\nselect(bill_depth_mm) |&gt;\ndeframe()\n\nIn polars the equivalent of this\n\nvec1 = penguins[\"bill_depth_mm\"]\n\nprint(vec1[0,1])\n\nshape: (2,)\nSeries: 'bill_depth_mm' [str]\n[\n    \"18.7\"\n    \"17.4\"\n]\n\n\n\nRPython\n\n\n\nvec1[1:3]\n\n[1] 18.7 17.4 18.0\n\n\n\n\n\nimport numpy as np \n\nprint(vec1[0:2])\n\nshape: (2,)\nSeries: 'bill_depth_mm' [str]\n[\n    \"18.7\"\n    \"17.4\"\n]"
  },
  {
    "objectID": "blog/2022/2021-11-25-what-to-do-when-you-break-r/index.html",
    "href": "blog/2022/2021-11-25-what-to-do-when-you-break-r/index.html",
    "title": "What to do when you break R",
    "section": "",
    "text": "Hi all, when I first stated using R I tried making my website using blogdown. While Alison Hill PhD provides an excellent intro to launching your website. However, I am truly special and managed to mess this process up. For a few days whenever I did anything more computationally intensive than\n\nrm(list=ls())\nlibrary(tidyverse)\n\nI would get a nasty error message saying “c stack usage is too close to the limit” and I could not do anything. This would have been fine but at the time I was still taking classes and need to have R working to complete the problem sets.\nSo what did I do to get in the c stack death spiral and how did it end up being fixed? For the former you should not skip steps in Dr. Hill’s post. For the later well to spare you the long arduous process here is what we tried.\n\nme consulting stackoverflow & realizing I really f****k up\nrestarting my computer\nuninstalling & reinstalling blogdown\nuninstalling & reinstalling R\nuninstalling & reinstalling pandocs\n\nAfter hours of trouble shooting #rstats twitter came to the rescue when this distress signal was sent out.\n\n\n#rstats world! I have a student who is getting a \"c stack usage is too close to the limit\" every time when using knitr (even with an R-chunk-free Rmd file) & getting same error when trying to install anything with devtools or remotes. We've un/reinstalled R but no luck. Ideas?\n\n— Andrew Heiss (🐘 @andrew@fediscience.org) (@andrewheiss) February 7, 2021\n\n\nSo here is what ended up working. To start you will need a super simple Rmd file to test with in a local directory. I suggest starting a new Rmd file with nothing in it other than the default YAML header and “test” in the main body or “Lorem Ipsum” if you feel fancy.\nIn the terminal run the following code\n\ncd ~ \n\nls -la\n\nThen look for files starting with a period. Okay if you messed up in the initial blogdown setup you are looking for the “.Rprofile” that is causing you the problem. What ended up happening is that you broke all of R by including a recursive function. So restarting and uninstalling R will not kill the function it will be there\n\n\n\nWhat you are going to do is open a terminal in Rstudio or otherwise than start running this.\n\ncat.Rprofile\n\nthan run\n\ncat.zshrc\n\nthan after that run\nmv. Rprofile .Rprofile-original\nThen close out Rstudio and reopen Rstudio. Then try to knit your super simple Rmd file and install a package and doing something fun! Than knit that file.\nHopefully the dreaded C stack usage error is gone. If it is than celebrate\n\n\n\nbecause you can use R again!!!!"
  },
  {
    "objectID": "CV/index.html",
    "href": "CV/index.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "blog/2024/python-notes/index.html",
    "href": "blog/2024/python-notes/index.html",
    "title": "R to Python: Just the basics",
    "section": "",
    "text": "The job hunt is in full effect and I am pushing myself to do more and more things in Python out of fear that that there will be no R option at any future job. I feel as if I have a relatively okay handle on Polars and am comfortable enough to do data cleaning when prompted. However, one of the pain points with me is that I generally have a low level of understanding of various things in Python. I am going to use this blog post as a living notebook on Python. I am bascially just going to work through the ‘Zero to Python Textbook’ and as much as possible translate it to equivalent R syntax. Right now when writing in Python I first have to translate things to R and then back again to make sure to make it make sense."
  },
  {
    "objectID": "blog/2024/python-notes/index.html#loops",
    "href": "blog/2024/python-notes/index.html#loops",
    "title": "R to Python: Just the basics",
    "section": "Loops",
    "text": "Loops\nOne of the things that is incredibly infuriating with python for me is that the indentation thing never made anysense. Why should we care about it other than making our code look pretty. The problem is that python uses indentation to denote code blocks in lieu of using {} like R or some general purpose programming languages like Java. So when we are looping over things in R we do\n\nfor(i in 1:5){\n\nprint(i^2)\n \n}\n\n[1] 1\n[1] 4\n[1] 9\n[1] 16\n[1] 25\n\n\nThis works becuase R isn’t relying on the spacing to tell it what code is in the loop. Whereas in python if we did\n\nfor i in range(1,5):\nprint(i**2)\n\nexpected an indented block after 'for' statement on line 1 (&lt;string&gt;, line 2)\n\n\nWe get an error. So if we make it a little bit more complicated we are using spacing to tell python what is inside a loop, function definition, etc\n\nfor i in range(1,6):\n    i = i * 3\n    if i%2 == 0:\n        print(f'{i} is even')\n    else:\n        print(f'{i} is odd')\n\n3 is odd\n6 is even\n9 is odd\n12 is even\n15 is odd\n\n\nOne thing I have never really understood is when and how to use else if so we should learn how to do this. I think if’s are fairly straight forward if something is true do it. Same with else if the if statment isn’t met doing something else. The best way I can explain it to myself is that else if is colloquilly more equivelent to or if\n\nvals = [1,3,4,5,56,7,78,9,7]\n\nfor i in vals:\n    if i % 2 == 0:\n        print(f'{i} is even')\n    elif i == 5:\n        print(f'{i} triggered the elif')\n    else:\n        print(f'{i} is odd')\n\n1 is odd\n3 is odd\n4 is even\n5 triggered the elif\n56 is even\n7 is odd\n78 is even\n9 is odd\n7 is odd"
  },
  {
    "objectID": "blog/2024/python-notes/index.html#tuples-and-lists",
    "href": "blog/2024/python-notes/index.html#tuples-and-lists",
    "title": "R to Python: Just the basics",
    "section": "Tuples and Lists",
    "text": "Tuples and Lists\nSo a tuple is a container that stores an ordered collection of items of the same or different primitives but is not mutable. So lets define a tuple and a list. Both have the same indexing syntax so you can index do regular and negative indexing.\n\ntp = (1, '2', 3, 2*2)\n\ntype(tp)\n\n&lt;class 'tuple'&gt;\n\nlt = [1,'2', 3, 2 *2 ]\n\ntype(lt)\n\n&lt;class 'list'&gt;\n\nprint('This is the first element of the  tuple', tp[0], 'this is the first element of the  list', lt[0])\n\nThis is the first element of the  tuple 1 this is the first element of the  list 1\n\nprint('this is the last element of the tuple', tp[-1], 'this is the last element of the list', lt[-1])\n\nthis is the last element of the tuple 4 this is the last element of the list 4\n\n\nAdditionally you can use slicing to grab a range of elements. One thing that feels weird as an R user is you can some interesting things like example 2\n\nslice_one = lt[1:4]\n\nslice_two = lt[1:2:4]\n\nprint(slice_two)\n\n['2']\n\n\nHowever, one of the major differences is if we wanted to change the underlying object. You can change the elements of a list but you cannot change the elements of a tuple\n\ntp[1] = 2\n\nTypeError: 'tuple' object does not support item assignment\n\nlt[1] = 2\n\nlt\n\n[1, 2, 3, 4]\n\n\nSo this will update the second element of the this. If we wanted to add things to a list we can simply do\n\n\nlt.append(5)\n\nIf we wanted to remove items from a list we would simply do\n\n\ndel lt[0]\n\nThe interesting thing about python is that lists are not neccessarily equivelent as vectors in R but we can do stuff we would normally would do with vectors\n\nRPython\n\n\n\nvec_one = c(1:10)\nvec_two = c(11:20)\n\n\nsum(c(vec_one, vec_two))\n\n[1] 210\n\n\n\n\n\nvec_one = list(range(1,11))\n\nvec_two = list(range(11,21))\n\nsum(vec_one + vec_two)\n\n210"
  },
  {
    "objectID": "blog/2024/python-notes/index.html#dictionaries",
    "href": "blog/2024/python-notes/index.html#dictionaries",
    "title": "R to Python: Just the basics",
    "section": "Dictionaries",
    "text": "Dictionaries\nDictionaries in Python hold key values pairs. Which as an R user is a little bit foreign since we don’t neccessarily have something that is exactly equivelent. The closest equivelent I could think of would be a named list or a named vector. But that isn’t neccessarily the same thing. One of the nice things about dicts is that you can reference things by the key, but something that is a bit weird is that you can’t really do it by index position. This is likely for a good reason, but just not someting I am used to. However, if you wanted like the first element of the first key you would just index it like a list since well the value of it is a list.\n\nmy_dict = {'fruits':['apples', 'pears'], 'numbers':[1,2,3]}\n\nmy_dict['fruits']\n\n['apples', 'pears']\n\nmy_dict['numbers']\n\n[1, 2, 3]\n\nmy_dict['fruits'][0]\n\n'apples'\n\n\nSo this definetly matters when we go and thing about iterating things. Since we have to use different syntaxes. So if you wanted to print out the all the items in a list then you could do this.\n\nfor i in lt:\n    print(i)\n\n2\n3\n4\n5\n\n\nhowever in a dictionary you only get the keys and not the values which was what I was looking for. You would have to do something like this.\n\nfor key, value in my_dict.items():\n    print(key, value)\n\nfruits ['apples', 'pears']\nnumbers [1, 2, 3]\n\n\nThis also matters when you want to add things or delete things. If we did something like this we are just overwriting the existing dictionary.\n\n\nmy_dict['fruits'] = 'mango'\n\nmy_dict['numbers'] = 100\n\nIf we wanted to actually add things without overwriting an existing dictionary you have lots of options which I will cover in the next sections. However we can start adding new key value pairs like this\n\nmy_dict['Cities'] = ['Atlanta','New York City', 'San Francisco']\n\n\nmy_dict\n\n{'fruits': 'mango', 'numbers': 100, 'Cities': ['Atlanta', 'New York City', 'San Francisco']}\n\n\nYou can also update the dictionary using update\n\nmy_dict.update({'States': ['Georgia', 'New York', 'California']})\n\nprint(my_dict)\n\n{'fruits': 'mango', 'numbers': 100, 'Cities': ['Atlanta', 'New York City', 'San Francisco'], 'States': ['Georgia', 'New York', 'California']}"
  },
  {
    "objectID": "blog/2024/python-notes/index.html#more-efficient-appending",
    "href": "blog/2024/python-notes/index.html#more-efficient-appending",
    "title": "R to Python: Just the basics",
    "section": "More efficient appending",
    "text": "More efficient appending\n\nList Compression\nI am skipping ahead a little bit but I wanted to learn this since I have only ever implemented but don’t have a full understanding of what is going on and when and why to use it. So lets say I wanted to make a new list and fill it with its square. The R user in me would do something like this\n\nnumbs &lt;- list(1, 2, 3, 4, 5,6,7,8,9,10)\n\n\nnumbs = lapply(numbs, \\(x) x * x)\n\nnumbs\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 9\n\n[[4]]\n[1] 16\n\n[[5]]\n[1] 25\n\n[[6]]\n[1] 36\n\n[[7]]\n[1] 49\n\n[[8]]\n[1] 64\n\n[[9]]\n[1] 81\n\n[[10]]\n[1] 100\n\n\nYou could do a similar thing in python.\n\nnumbs = []\n\nfor i in range(10):\n    numbs.append(i * i)\n\n\nnumbs\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\nOne of the problems that you can run into is that for a lot of stuff growing a list can take awhile. In R that is why we tend to prefer using functions along with lapply or map over for loops. So if we wanted to convert some temperatures from farenheit to celsius we would generally prefer to write a function and then apply it to a list rather than use a for loop to do this. Python has a few more tricks up its sleeve to accomplish this. If we wanted a straight forward translation from the tidyverse to python we could do.\n\ndef temp_converter(temp):\n     return (temp-32) *5/9\n\n\ntemp_list = [32, 212, 100]\n\n\nc = map(temp_converter, temp_list)\n\nlist(c)\n\n[0.0, 100.0, 37.77777777777778]\n\n\nthis is totally fine! But there are some unncessary intermediate steps and really just more me trying to force it into my tiny little functional programming mind. Instead we can use list comprehesion to speed this process up and is more in line with python.\n\nc = [temp_converter(temp_list) for temp_list in temp_list]\n\nc\n\n[0.0, 100.0, 37.77777777777778]\n\n\nOne of the benefits of this is that you can add control flows to really quickly and really flexible change elements of a list. So lets say that some celcius that leaks into our little list. Normally we would want to address this leakage, but for pedagocical purposes lets just add control flows.\n\ntemp_list = [32, 212, 100, 0] \n\n\nc = [temp_converter(temp_list) for temp_list in temp_list if temp_list not in [0,100]]\n\n\nc\n\n[0.0, 100.0]\n\n\n\n\nUpdating dictionaries\nSo in the last section we learned that updating dictionaries is a bit more delicate. One big thing that you have to keep in mind is the types within the dictionary. So our dictionary is really just two little lists with a dictionary trench coat. So we have to use list appending. So lets do that.\n\n\nnew_dict = {'fruits': ['watermelons', 'strawberries'],  'numbers' : [4,5,6]}\n\nnew_dict['fruits'].append('cherry')\n\nnew_dict['numbers'].append(7)\n\nThis is fine but not neccessarily the most efficient way to do things outside of canned examples. The most likely case is that we have a new dictionary to help us update things.\n\n\nupdate_dict = {'fruits': ['mangos', 'rasberry', 'jackfruit'], 'numbers':[2,4,5,6,7]}\n\nnew_dict['fruits'].extend(update_dict['fruits'])\n\nSince our dictionaries hold lists we could also theoretically use list comprehesion like this.\n\nnew_dict['numbers'] = [temp_converter(num) for num in new_dict['numbers']]\n\nnew_dict\n\n{'fruits': ['watermelons', 'strawberries', 'cherry', 'mangos', 'rasberry', 'jackfruit'], 'numbers': [-15.555555555555555, -15.0, -14.444444444444445, -13.88888888888889]}\n\n\nWe can also combine dictionaries using the | operator\n\njosh_vals = {'Name': 'Josh Allen', 'Location' : 'Atlanta', 'job': 'Grad Student'}\n\ngeorgia_vals = {'Nickname': 'Peach State', 'mascot': 'White Tailed Dear', 'Power 5 Schools': ['UGA', 'Georgia Tech']}\n\njosh_vals | georgia_vals\n\n{'Name': 'Josh Allen', 'Location': 'Atlanta', 'job': 'Grad Student', 'Nickname': 'Peach State', 'mascot': 'White Tailed Dear', 'Power 5 Schools': ['UGA', 'Georgia Tech']}\n\n\nWe can also modify in place using a special operator\n\njosh_vals |= georgia_vals\n\nIf we wanted to keep it within a Python framework we can use something called dictionary compression were we can do something like this\n\n\nsquares_dict = {i: i**2 for i in range(1,11)}\n\nOr if we had somethign like this\n\nmy_dict = {'United States' : 'Washington DC', 'France' : 'Paris', 'Italy' : 'Rome'}\n\n{capital: country for country, capital in my_dict.items()}\n\n{'Washington DC': 'United States', 'Paris': 'France', 'Rome': 'Italy'}"
  },
  {
    "objectID": "blog/2024/python-notes/index.html#class-instance-and-static-methods",
    "href": "blog/2024/python-notes/index.html#class-instance-and-static-methods",
    "title": "R to Python: Just the basics",
    "section": "Class, instance, and static methods",
    "text": "Class, instance, and static methods\nOne way we can do this is enforcing types in our functions. There are several flavors which can be used to enforce various behaviours\n\nInstance methods\nInstance methods are the most common and are really just functions defined in the body of the class. However they take on a different flavor when we are using them inside the body of the class\n\nclass Dog:\n    def __init__(self, age: int, name:str):\n        if not isinstance(name, str):\n            raise ValueError(\"Name must be a string\")\n        self.name = name\n        if not isinstance(age, int):\n            raise ValueError(\"Age must be an integer\")\n        self.age  = age\n    def description(self, age:int, name:str):\n        print(f\"{self.name} is {self.age} old\")\n\n\ndoggo = Dog(name = 7, age = 'This is a string')\n\nValueError: Name must be a string\n\n\nNow our class is a little bit more robust we can’t start passing things off that we shouldn’t. One thing that we can do is actually bypass the the name/age checks\n\ndog = Dog(age=5, name=\"Buddy\")\n\ndog.description(age = 'five', name = 123)\n\nBuddy is 5 old\n\n\nOne thing that you will notice is that the description doesn’t care about these violations. Which is not ideal so to make this even more robust we can add some fairly simple checks\n\nclass Dog:\n    def __init__(self, name:str, age:int):\n        if not isinstance(name, str):\n            raise ValueError(\"name should be a string\")\n        self.name = name\n        if not isinstance(age, int) or age &lt;= 0:\n            raise ValueError(\"age should be an integer or be greater than 1\")\n        self.age = age\n    def description(self, name, age):\n        if age != self.age or name != self.name:\n            raise ValueError(\"Provided age or name does not match dog attributes\")\n        print(f\"{self.name} is {self.age} years old\")\n\n\ndog = Dog(age=5, name=\"Buddy\")\n\ndog.description(age = 'five', name = 123)\n\nValueError: Provided age or name does not match dog attributes"
  },
  {
    "objectID": "blog/2024/translating-tidyverse-to-sql/index.html",
    "href": "blog/2024/translating-tidyverse-to-sql/index.html",
    "title": "Tidyverse to SQL",
    "section": "",
    "text": "Since I am going on the non-academic job market it is high time I learned SQL. I have tried lots of amazing resources but find it hard for me to navigate between notes and various and learning SQL since they are just familiar enough to trip me up and lots of them send you off to various editors. This blog post will serve as my notes and hopefully as a resource for not myself. The general idea is I am just going to work through R4DS and the various dplyr verbs. Then move onto some more advanced SQL stuff like window functions and what not."
  },
  {
    "objectID": "blog/2024/translating-tidyverse-to-sql/index.html#setup",
    "href": "blog/2024/translating-tidyverse-to-sql/index.html#setup",
    "title": "Tidyverse to SQL",
    "section": "Setup",
    "text": "Setup\nFor the majority of this palmerpenguins dataset not because you really need to use SQL for a dataset this small but copying over the nyc-taxi dataset is incredibly annoying for blogging purposes.\n\nlibrary(DBI)\nlibrary(arrow)\n\n\nAttaching package: 'arrow'\n\n\nThe following object is masked from 'package:utils':\n\n    timestamp\n\nlibrary(dbplyr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ lubridate::duration() masks arrow::duration()\n✖ dplyr::filter()       masks stats::filter()\n✖ dplyr::ident()        masks dbplyr::ident()\n✖ dplyr::lag()          masks stats::lag()\n✖ dplyr::sql()          masks dbplyr::sql()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\npengs = palmerpenguins::penguins\n\ncon =  src_memdb()\n\npengs = copy_to(con, pengs,\n       overwrite = TRUE)\n\nWe are going to go back and forth using dbplyr and SQL to query the dataset. What impressed me throughout this process was how seamless dbplyr works with dplyr verbs work. With the exception of some string functions it can work as a drop in replacement for SQL. What really helped throughout this process was writing out my queries and using show_query.\n\npengs |&gt;\n    select(species) |&gt;\n    show_query()\n\n&lt;SQL&gt;\nSELECT `species`\nFROM `pengs`\n\n\nWhich will give us a SQL query. Obviously this is a pretty simple query but as we get more and more complex this is going to be helpful. For the most part show_query outputs the right query but can be a little bit difficult to debug because of the limitations of showing things in the R console."
  },
  {
    "objectID": "blog/2024/translating-tidyverse-to-sql/index.html#select",
    "href": "blog/2024/translating-tidyverse-to-sql/index.html#select",
    "title": "Tidyverse to SQL",
    "section": "Select",
    "text": "Select\nOne convention in SQL which I don’t really get but is a thing is that functions are defined using all caps. Luckily for us the SQL and dplyr versions are pretty much the same one is just shouty. If we wanted all the columns like we may when we are importing the dataset for the first time we are just going to do SELECT * FROM taxis. There is really not like a perfect equivalent in R except for maybe head. But even then it is not a perfect one to one."
  },
  {
    "objectID": "blog/2024/translating-tidyverse-to-sql/index.html#r",
    "href": "blog/2024/translating-tidyverse-to-sql/index.html#r",
    "title": "Tidyverse to SQL",
    "section": "R",
    "text": "R\n\nhead(pengs)\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.51.0 [:memory:]\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "blog/2024/translating-tidyverse-to-sql/index.html#sql",
    "href": "blog/2024/translating-tidyverse-to-sql/index.html#sql",
    "title": "Tidyverse to SQL",
    "section": "SQL",
    "text": "SQL\n\ntbl(con, sql(\"SELECT * FROM pengs\"))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.51.0 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "blog/2024/translating-tidyverse-to-sql/index.html#filter",
    "href": "blog/2024/translating-tidyverse-to-sql/index.html#filter",
    "title": "Tidyverse to SQL",
    "section": "Filter",
    "text": "Filter\nThe first major difference syntactically between dplyr and SQL is with filter statements aka WHERE statements in SQL. So let’s say we want only penguins that are Adelie penguins.\n\nRSQL\n\n\n\npengs |&gt;\n    filter(species == 'Adelie')\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.51.0 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\nBecomes.\n\n\n\ntbl(con,sql( \"\n    SELECT * from pengs\n    WHERE species = 'Adelie'\n\"))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.51.0 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\n\n\n\n\n\nSome flavors of SQL make you end lines with ‘;’\nAs dplyr users will notice the way we specified the equality position uses the = instead of ==. This is going to come up a lot. The same thing goes for negation operations.\n\nRSQL\n\n\n\npengs |&gt;\n    filter(species != 'Adelie')\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.51.0 [:memory:]\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo  Biscoe           46.1          13.2               211        4500\n 2 Gentoo  Biscoe           50            16.3               230        5700\n 3 Gentoo  Biscoe           48.7          14.1               210        4450\n 4 Gentoo  Biscoe           50            15.2               218        5700\n 5 Gentoo  Biscoe           47.6          14.5               215        5400\n 6 Gentoo  Biscoe           46.5          13.5               210        4550\n 7 Gentoo  Biscoe           45.4          14.6               211        4800\n 8 Gentoo  Biscoe           46.7          15.3               219        5200\n 9 Gentoo  Biscoe           43.3          13.4               209        4400\n10 Gentoo  Biscoe           46.8          15.4               215        5150\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\n\n\n\ntbl(con, sql(\"SELECT * from pengs \n             WHERE NOT species = 'Adelie'\"))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.51.0 [:memory:]\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo  Biscoe           46.1          13.2               211        4500\n 2 Gentoo  Biscoe           50            16.3               230        5700\n 3 Gentoo  Biscoe           48.7          14.1               210        4450\n 4 Gentoo  Biscoe           50            15.2               218        5700\n 5 Gentoo  Biscoe           47.6          14.5               215        5400\n 6 Gentoo  Biscoe           46.5          13.5               210        4550\n 7 Gentoo  Biscoe           45.4          14.6               211        4800\n 8 Gentoo  Biscoe           46.7          15.3               219        5200\n 9 Gentoo  Biscoe           43.3          13.4               209        4400\n10 Gentoo  Biscoe           46.8          15.4               215        5150\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\n\n\n\nIf we want multiple conditions in our where statements instead of | or &/, we actually just use the words or and and\n\nRSQL\n\n\n\npengs |&gt;\n    filter(species == 'Chinstrap' | species == 'Adelie')\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.51.0 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\nbecomes\n\n\n\ntbl(con, sql(\"SELECT * from pengs \n            WHERE species = 'Adelie' OR species = 'Chinstrap'\"))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.51.0 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\n\n\n\nYou could easily sub in AND but that feels a bit excessive to continue this process for each possible combination. One thing that I do all the time is use sets to subset my data.\n\nRSQL\n\n\n\npengs |&gt;\n    filter(species %in% c('Chinstrap', \"Gentoo\"))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.51.0 [:memory:]\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo  Biscoe           46.1          13.2               211        4500\n 2 Gentoo  Biscoe           50            16.3               230        5700\n 3 Gentoo  Biscoe           48.7          14.1               210        4450\n 4 Gentoo  Biscoe           50            15.2               218        5700\n 5 Gentoo  Biscoe           47.6          14.5               215        5400\n 6 Gentoo  Biscoe           46.5          13.5               210        4550\n 7 Gentoo  Biscoe           45.4          14.6               211        4800\n 8 Gentoo  Biscoe           46.7          15.3               219        5200\n 9 Gentoo  Biscoe           43.3          13.4               209        4400\n10 Gentoo  Biscoe           46.8          15.4               215        5150\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\nBecomes\n\n\n\ntbl(con, sql(\"SELECT * from pengs\n            WHERE species IN ('Chinstrap', 'Gentoo')\"))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.51.0 [:memory:]\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo  Biscoe           46.1          13.2               211        4500\n 2 Gentoo  Biscoe           50            16.3               230        5700\n 3 Gentoo  Biscoe           48.7          14.1               210        4450\n 4 Gentoo  Biscoe           50            15.2               218        5700\n 5 Gentoo  Biscoe           47.6          14.5               215        5400\n 6 Gentoo  Biscoe           46.5          13.5               210        4550\n 7 Gentoo  Biscoe           45.4          14.6               211        4800\n 8 Gentoo  Biscoe           46.7          15.3               219        5200\n 9 Gentoo  Biscoe           43.3          13.4               209        4400\n10 Gentoo  Biscoe           46.8          15.4               215        5150\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\n\n\n\nin this case we define a set in a similar way. If we wanted to negate this statement all we would do is\n\ntbl(con, sql(\"SELECT * from pengs\n            WHERE NOT species IN ('Chinstrap', 'Gentoo')\"))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.51.0 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\nLets say we want to find penguins that are less than the average body mass in R this is fairly straightforward\n\npengs |&gt;\n    filter(body_mass_g &lt; mean(body_mass_g, na.rm = TRUE))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.51.0 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           36.7          19.3               193        3450\n 5 Adelie  Torgersen           39.3          20.6               190        3650\n 6 Adelie  Torgersen           38.9          17.8               181        3625\n 7 Adelie  Torgersen           34.1          18.1               193        3475\n 8 Adelie  Torgersen           37.8          17.1               186        3300\n 9 Adelie  Torgersen           37.8          17.3               180        3700\n10 Adelie  Torgersen           41.1          17.6               182        3200\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\nHowever when we do this in some flavor of SQL it is not as straightforward. These are aggregation functions that where can’t handle because thats not its job. So if we did\n\ntbl(con, \"SELECT * from pengs WHERE body_mass_g &lt; AVG(body_mass_g)\")\n\nError in `db_query_fields.DBIConnection()`:\n! Can't query fields.\nℹ Using SQL: SELECT * FROM `SELECT * from pengs WHERE body_mass_g &lt;\n  AVG(body_mass_g)` AS `q01` WHERE (0 = 1)\nCaused by error:\n! no such table: SELECT * from pengs WHERE body_mass_g &lt; AVG(body_mass_g)\n\n\nWe get an error. If we wanted to use aggregation functions we have to change how we do this\n\npengs |&gt;\n    filter(body_mass_g &lt; mean(body_mass_g, na.rm = TRUE)) |&gt;\n    show_query()\n\n&lt;SQL&gt;\nSELECT\n  `species`,\n  `island`,\n  `bill_length_mm`,\n  `bill_depth_mm`,\n  `flipper_length_mm`,\n  `body_mass_g`,\n  `sex`,\n  `year`\nFROM (\n  SELECT `pengs`.*, AVG(`body_mass_g`) OVER () AS `col01`\n  FROM `pengs`\n) AS `q01`\nWHERE (`body_mass_g` &lt; `col01`)\n\n\nWhat is this OVER thing? OVER in SQL is a window function. There is a more technical way to explain this but heuristically when we pass AVG to WHERE we are effectively doing this. So there is not really anything to compare it too.\n\npengs |&gt;\n    summarise(mean(body_mass_g, na.rm = TRUE))\n\n# Source:   SQL [?? x 1]\n# Database: sqlite 3.51.0 [:memory:]\n  `mean(body_mass_g, na.rm = TRUE)`\n                              &lt;dbl&gt;\n1                             4202.\n\n\nIf we wanted to filter penguins that are less than the average body mass we have to prevent this aggregation process by creating a column and then creating a less than statement like this\n\ntbl(con, sql(\"SELECT * FROM(\n              SELECT pengs .*, AVG(body_mass_g) OVER () AS avg\n               FROM pengs)\n              WHERE (body_mass_g &lt; avg)\"))\n\n# Source:   SQL [?? x 9]\n# Database: sqlite 3.51.0 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           36.7          19.3               193        3450\n 5 Adelie  Torgersen           39.3          20.6               190        3650\n 6 Adelie  Torgersen           38.9          17.8               181        3625\n 7 Adelie  Torgersen           34.1          18.1               193        3475\n 8 Adelie  Torgersen           37.8          17.1               186        3300\n 9 Adelie  Torgersen           37.8          17.3               180        3700\n10 Adelie  Torgersen           41.1          17.6               182        3200\n# ℹ more rows\n# ℹ 3 more variables: sex &lt;chr&gt;, year &lt;int&gt;, avg &lt;dbl&gt;\n\n\nIt is a little clunky but the tl;dr is that we basically have two FROM statements so if we wanted all penguins between the minimum and the average we could do\n\nRSQL\n\n\n\npalmerpenguins::penguins |&gt;\n    filter(between(body_mass_g, left = min(body_mass_g, na.rm = TRUE), right = mean(body_mass_g, na.rm = TRUE)))\n\n# A tibble: 193 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           36.7          19.3               193        3450\n 5 Adelie  Torgersen           39.3          20.6               190        3650\n 6 Adelie  Torgersen           38.9          17.8               181        3625\n 7 Adelie  Torgersen           34.1          18.1               193        3475\n 8 Adelie  Torgersen           37.8          17.1               186        3300\n 9 Adelie  Torgersen           37.8          17.3               180        3700\n10 Adelie  Torgersen           41.1          17.6               182        3200\n# ℹ 183 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\ntbl(con, sql(\"SELECT * FROM(\n             SELECT pengs .*, AVG(body_mass_g) OVER() AS avg, MIN(body_mass_g) OVER() AS min\n            FROM pengs)\n            WHERE body_mass_g BETWEEN min AND avg\"))\n\n# Source:   SQL [?? x 10]\n# Database: sqlite 3.51.0 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           36.7          19.3               193        3450\n 5 Adelie  Torgersen           39.3          20.6               190        3650\n 6 Adelie  Torgersen           38.9          17.8               181        3625\n 7 Adelie  Torgersen           34.1          18.1               193        3475\n 8 Adelie  Torgersen           37.8          17.1               186        3300\n 9 Adelie  Torgersen           37.8          17.3               180        3700\n10 Adelie  Torgersen           41.1          17.6               182        3200\n# ℹ more rows\n# ℹ 4 more variables: sex &lt;chr&gt;, year &lt;int&gt;, avg &lt;dbl&gt;, min &lt;int&gt;\n\n\n\n\n\nIf you notice in all our examples, we have lots and lots of missing values. This is one of the most common tasks in like any data science task. Let’s say that we can safely ignore the missing valus. In R we have a lot of options whether we are using filter or drop_na from tidyr. However, in SQL missing values are usually represented by NULL\n\ntbl(con, sql(\"SELECt * FROM pengs \n                WHERE NOT sex IS NULL\"))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.51.0 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           36.7          19.3               193        3450\n 5 Adelie  Torgersen           39.3          20.6               190        3650\n 6 Adelie  Torgersen           38.9          17.8               181        3625\n 7 Adelie  Torgersen           39.2          19.6               195        4675\n 8 Adelie  Torgersen           41.1          17.6               182        3200\n 9 Adelie  Torgersen           38.6          21.2               191        3800\n10 Adelie  Torgersen           34.6          21.1               198        4400\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "blog/2024/translating-tidyverse-to-sql/index.html#rename",
    "href": "blog/2024/translating-tidyverse-to-sql/index.html#rename",
    "title": "Tidyverse to SQL",
    "section": "Rename",
    "text": "Rename\nThe AS function is kind the work horse for the next few sections. The naming convention differs a little bit so instead of new_name = old_name we do SELECT old_name as new_name\n\ntbl(con, sql(\"SELECT species AS kinds_of_penguins\n          FROM pengs\"))\n\n# Source:   SQL [?? x 1]\n# Database: sqlite 3.51.0 [:memory:]\n   kinds_of_penguins\n   &lt;chr&gt;            \n 1 Adelie           \n 2 Adelie           \n 3 Adelie           \n 4 Adelie           \n 5 Adelie           \n 6 Adelie           \n 7 Adelie           \n 8 Adelie           \n 9 Adelie           \n10 Adelie           \n# ℹ more rows"
  },
  {
    "objectID": "blog/2024/translating-tidyverse-to-sql/index.html#mutate",
    "href": "blog/2024/translating-tidyverse-to-sql/index.html#mutate",
    "title": "Tidyverse to SQL",
    "section": "Mutate",
    "text": "Mutate\nAs lots of things go we need to be able to create our own variables. So to do this in R we do this\n\npengs |&gt;\n    mutate(sqr_body_mass = body_mass_g^2)\n\n# Source:   SQL [?? x 9]\n# Database: sqlite 3.51.0 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 3 more variables: sex &lt;chr&gt;, year &lt;int&gt;, sqr_body_mass &lt;dbl&gt;\n\n\nIn SQL to get the equivalent statement we use SELECT transformation AS new_var_name when we need to do things that are not in the dataset. So we basically need to define the column before we do anything.\n\ntbl(con, sql(\"SELECT pengs .*, POWER(body_mass_g,2) AS sqr_body_mass\n            FROM pengs\"))\n\n# Source:   SQL [?? x 9]\n# Database: sqlite 3.51.0 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 3 more variables: sex &lt;chr&gt;, year &lt;int&gt;, sqr_body_mass &lt;dbl&gt;\n\n\nSo if we needed wanted to make a ratio of bill depth to bill length we would do\n\ntbl(con, sql(\"SELECT pengs .*, bill_depth_mm/bill_length_mm AS ratio \n            FROM pengs\"))\n\n# Source:   SQL [?? x 9]\n# Database: sqlite 3.51.0 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 3 more variables: sex &lt;chr&gt;, year &lt;int&gt;, ratio &lt;dbl&gt;\n\n\nA very important thing we do all the time is generate indicator variables for treatment status gender etc. Oddly enough if we peep the output of show query we see a familiar face!"
  },
  {
    "objectID": "blog/2024/translating-tidyverse-to-sql/index.html#r-7",
    "href": "blog/2024/translating-tidyverse-to-sql/index.html#r-7",
    "title": "Tidyverse to SQL",
    "section": "R",
    "text": "R\n\npengs |&gt;\n    mutate(male = ifelse(sex == 'Male', 1, 0)) |&gt;\n    show_query()\n\n&lt;SQL&gt;\nSELECT\n  `pengs`.*,\n  CASE WHEN (`sex` = 'Male') THEN 1.0 WHEN NOT (`sex` = 'Male') THEN 0.0 END AS `male`\nFROM `pengs`\n\n\nSo to make an indicator variable we would just do\n\ntbl(con, sql(\"SELECT pengs.*, CASE WHEN (sex = 'male') THEN 1.0 WHEN not (sex = 'male') THEN 0.0 END AS male\n             FROM pengs\"))\n\n# Source:   SQL [?? x 9]\n# Database: sqlite 3.51.0 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 3 more variables: sex &lt;chr&gt;, year &lt;int&gt;, male &lt;dbl&gt;\n\n\nLet’s combine our window functions with our friend case_when\n\ntbl(con, sql(\"SELECT * FROM(SELECT pengs .*,\n           AVG(body_mass_g) AS avg, MIN(body_mass_g) AS min, MAX(body_mass_g) AS max,\n            CASE WHEN (body_mass_g = min) THEN 'This penguins is small' WHEN (body_mass_g = avg) THEN 'This is an average sized penguin' WHEN (body_mass_g = max) THEN 'this is a really big penguin' END AS note \n            FROM pengs)\"))\n\nI will spare you the long output of the error message. But needless to say this was wrong. If we translate what I was trying to do into dplyr we get this\n\npengs |&gt;\n    mutate(note = case_when(\n            body_mass_g == min(body_mass_g) ~ 'This is a small peng',\n            body_mass_g == mean(body_mass_g) ~ 'Average sized peng',\n            body_mass_g == max(body_mass_g) ~ 'Big sized peng',\n             .default = 'Penguin is some size')) |&gt;\n        show_query()\n\nWarning: Missing values are always removed in SQL aggregation functions.\nUse `na.rm = TRUE` to silence this warning\nThis warning is displayed once every 8 hours.\n\n\n&lt;SQL&gt;\nSELECT\n  `pengs`.*,\n  CASE\nWHEN (`body_mass_g` = MIN(`body_mass_g`) OVER `win1`) THEN 'This is a small peng'\nWHEN (`body_mass_g` = AVG(`body_mass_g`) OVER `win1`) THEN 'Average sized peng'\nWHEN (`body_mass_g` = MAX(`body_mass_g`) OVER `win1`) THEN 'Big sized peng'\nELSE 'Penguin is some size'\nEND AS `note`\nFROM `pengs`\nWINDOW `win1` AS ()\n\n\nSo it looks like we need to change the window function\n\ncheck = tbl(con, sql(\"SELECT pengs .*,\n              CASE\n            WHEN (body_mass_g &gt;= MIN(body_mass_g) OVER win1) THEN 'this is a small penguin'\n            WHEN (body_mass_g = AVG(body_mass_g) OVER win1) THEN 'this is an average sized penguin'\n            WHEN (body_mass_g = MAX(body_mass_g) OVER win1) THEN 'this is a big penguin'\n            ELSE 'This penguin is not big, small or average'\n            END AS note\n            FROM pengs \n            WINDOW win1 AS ()\")) |&gt;\n                collect()\n\nLets look at this a little closer to make sure this worked. We would probably want to make this a little more robust. So lets go ahead and define a range.\n\ntbl(con, sql(\"SELECT pengs .*,\n              CASE\n            WHEN (body_mass_g &gt;= MIN(body_mass_g) OR body_mass_g &lt; AVG(body_mass_g)  OVER win1) THEN 'this is a small penguin'\n            WHEN (body_mass_g &gt;= AVG(body_mass_g) OR body_mass_g &lt; MAX(body_mass_G) OVER win1) THEN 'this is an average sized penguin'\n            WHEN (body_mass_g &gt;= MAX(body_mass_g) OVER win1) THEN 'this is a big penguin'\n            ELSE 'This penguin is not big, small or average'\n            END AS note\n            FROM pengs \n            WINDOW win1 AS ()\"))\n\n# Source:   SQL [?? x 9]\n# Database: sqlite 3.51.0 [:memory:]\n  species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;     &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Chinstrap Dream            46.9          16.6               192        2700\n# ℹ 3 more variables: sex &lt;chr&gt;, year &lt;int&gt;, note &lt;chr&gt;"
  },
  {
    "objectID": "blog/2024/translating-tidyverse-to-sql/index.html#group-by-and-summarize",
    "href": "blog/2024/translating-tidyverse-to-sql/index.html#group-by-and-summarize",
    "title": "Tidyverse to SQL",
    "section": "Group by and summarize",
    "text": "Group by and summarize\nAs established earlier we can use SQL to summarize like this.\n\ntbl(con, sql('SELECT AVG(bill_depth_mm) AS avg\n           FROM pengs'))\n\n# Source:   SQL [?? x 1]\n# Database: sqlite 3.51.0 [:memory:]\n    avg\n  &lt;dbl&gt;\n1  17.2\n\n\nBut the actual practical utility is somewhat limited. Often we want group specific differences. Oddly enough I expected this to be a window function thing, but we actually delay computing of the mean by different groups to the end. I guess this makes sense if we are dealing with big data\n\ntbl(con, sql(\"SELECT species, AVG(body_mass_g) AS avg_body_mass\n            FROM pengs\n            GROUP BY species\"))\n\n# Source:   SQL [?? x 2]\n# Database: sqlite 3.51.0 [:memory:]\n  species   avg_body_mass\n  &lt;chr&gt;             &lt;dbl&gt;\n1 Adelie            3701.\n2 Chinstrap         3733.\n3 Gentoo            5076.\n\n\nSo if we wanted to count of the species we would do something along this line\n\ntbl(con, sql(\"SELECT species, COUNT(species) AS total\n            FROM pengs \n            GROUP BY species\"))\n\n# Source:   SQL [?? x 2]\n# Database: sqlite 3.51.0 [:memory:]\n  species   total\n  &lt;chr&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\nFor multiple grouping variables we would define the grouping variables the same way as we would in dplyr\n\ntbl(con, sql(\"SELECT species, sex, COUNT(species) AS total\n            FROM pengs \n            GROUP BY species, sex\"))\n\n# Source:   SQL [?? x 3]\n# Database: sqlite 3.51.0 [:memory:]\n  species   sex    total\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;\n1 Adelie    &lt;NA&gt;       6\n2 Adelie    female    73\n3 Adelie    male      73\n4 Chinstrap female    34\n5 Chinstrap male      34\n6 Gentoo    &lt;NA&gt;       5\n7 Gentoo    female    58\n8 Gentoo    male      61\n\n\nThe same would go for multiple summary functions\n\ntbl(con, sql(\"SELECT species, COUNT(species) AS total, AVG(bill_depth_mm) AS avg_bill_depth, MEDIAN(bill_depth_mm) AS median_bill_depth\n             FROM pengs \n             GROUP BY sex\"))\n\n# Source:   SQL [?? x 4]\n# Database: sqlite 3.51.0 [:memory:]\n  species total avg_bill_depth median_bill_depth\n  &lt;chr&gt;   &lt;int&gt;          &lt;dbl&gt;             &lt;dbl&gt;\n1 Adelie     11           16.6              17.1\n2 Adelie    165           16.4              17  \n3 Adelie    168           17.9              18.4"
  },
  {
    "objectID": "blog/2024/translating-tidyverse-to-sql/index.html#joinsappending-rows",
    "href": "blog/2024/translating-tidyverse-to-sql/index.html#joinsappending-rows",
    "title": "Tidyverse to SQL",
    "section": "Joins/Appending Rows",
    "text": "Joins/Appending Rows\nIn the real world it is rare that we will have all our data in one place. Companies keep information in lots of different places because well it would be bad if we kept credit card information with all the necessary components to make a purchase. Instead of having to figure out three different things malicious actors would just need to access one database. Replacing entire data tables can also skyrocket costs. So instead, it is more efficient to simply insert rows.\n\nApppending Rows\nTo kind of mimic this we are just going to slice this data frame roughly in half. While not entirely realistic the general process will be similar enough\n\n\nCode\npengs_top = palmerpenguins::penguins |&gt;\n    slice(1:172)\n\npengs_bottom = palmerpenguins::penguins |&gt;\n    slice(173:344)\n\ncon2 = src_memdb()\n\ncon3 = src_memdb()\n\npengs_top = copy_to(con2, pengs_top)\n\npengs_bottom = copy_to(con3, pengs_bottom)\n\n\nFor whatever reason show_query is not working with this so we are going to have to consult the interwebs. The SQL equivalent of bind_rows is UNION.\n\ntbl(con2, sql(\"SELECT * FROM pengs_top\n             UNION ALL \n             SELECT * FROM pengs_bottom\"))\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.51.0 [:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\nOne of the key things in this query is ALL which is somewhat new to me. Basically the ALL tells SQL that we don’t really care about duplicates so just add the rows regardless. So if we wanted to exclude duplicates we would do something like this\n\n\nCode\ntbl(con2, sql(\"SELECt * FROM pengs_top \n              UNION \n              SELECT * FROM pengs_top\")) |&gt;\n                collect() |&gt;\n                nrow()\n\n\n[1] 172\n\n\nCode\ntbl(con2,sql(\"SELECT * FROM pengs_top\") ) |&gt;\n    collect() |&gt;\n    nrow()\n\n\n[1] 172\n\n\n\n\nJoins\nLuckily for us the join syntax from dplyr is pretty directly taken SQL so lefts create some dummy data to join.\n\n\nCode\nnational_data &lt;- tribble(\n  ~state, ~year, ~unemployment, ~inflation, ~population,\n  \"GA\",   2018,  5,             2,          100,\n  \"GA\",   2019,  5.3,           1.8,        200,\n  \"GA\",   2020,  5.2,           2.5,        300,\n  \"NC\",   2018,  6.1,           1.8,        350,\n  \"NC\",   2019,  5.9,           1.6,        375,\n  \"NC\",   2020,  5.3,           1.8,        400,\n  \"CO\",   2018,  4.7,           2.7,        200,\n  \"CO\",   2019,  4.4,           2.6,        300,\n  \"CO\",   2020,  5.1,           2.5,        400\n)\n\nnational_libraries &lt;- tribble(\n  ~state, ~year, ~libraries, ~schools,\n  \"CO\",   2018,  230,        470,\n  \"CO\",   2019,  240,        440,\n  \"CO\",   2020,  270,        510,\n  \"NC\",   2018,  200,        610,\n  \"NC\",   2019,  210,        590,\n  \"NC\",   2020,  220,        530,\n)\n\ncon3 = src_memdb()\n\ncon4 = src_memdb()\n\nnational_data = copy_to(con4, national_data, overwrite = TRUE)\n\nnational_libraries = copy_to(con3, national_libraries, overwrite = TRUE)\n\n\nSo we have some fake national level data that we would like to join in to the dataset. We could do something like this but what we notice is that it is going to decide the join keys for us and probably create some headaches for us later on. To solve this we need to use our keys if we expose the underlying logic\n\nnational_data |&gt;\n    left_join(national_libraries, join_by(state, year)) |&gt;\n    show_query()\n\n&lt;SQL&gt;\nSELECT `national_data`.*, `libraries`, `schools`\nFROM `national_data`\nLEFT JOIN `national_libraries`\n  ON (\n    `national_data`.`state` = `national_libraries`.`state` AND\n    `national_data`.`year` = `national_libraries`.`year`\n  )\n\n\nWe will notice that join_by is shorthand for equality joins. What changes is that instead of left_key = right_key we have to specify what is coming from what table using .\n\ndb_con = con4$con\n\nquery = \"SELECT *\n             FROM national_data\n             LEFT JOIN national_libraries\n             ON (\n             national_data.state = national_libraries.state AND\n             national_data.year = national_libraries.year\n             )\n             \"\n\ndbGetQuery(db_con, sql(query))       \n\n  state year unemployment inflation population state year libraries schools\n1    GA 2018          5.0       2.0        100  &lt;NA&gt;   NA        NA      NA\n2    GA 2019          5.3       1.8        200  &lt;NA&gt;   NA        NA      NA\n3    GA 2020          5.2       2.5        300  &lt;NA&gt;   NA        NA      NA\n4    NC 2018          6.1       1.8        350    NC 2018       200     610\n5    NC 2019          5.9       1.6        375    NC 2019       210     590\n6    NC 2020          5.3       1.8        400    NC 2020       220     530\n7    CO 2018          4.7       2.7        200    CO 2018       230     470\n8    CO 2019          4.4       2.6        300    CO 2019       240     440\n9    CO 2020          5.1       2.5        400    CO 2020       270     510\n\n\n\n\nFor whatever reason with SQLite gets a little grumpy with the join syntax.\nIf we wanted to do various other joins like inner and anti joins we would do a similar thing.\n\nquery = \"SELECT * \n        FROM national_data\n    INNER JOIN national_libraries \n    ON(\n    national_data.state = national_libraries.state AND\n    national_data.year = national_libraries.year\n    )\n\"\n\ndbGetQuery(db_con, sql(query))\n\n  state year unemployment inflation population state year libraries schools\n1    CO 2018          4.7       2.7        200    CO 2018       230     470\n2    CO 2019          4.4       2.6        300    CO 2019       240     440\n3    CO 2020          5.1       2.5        400    CO 2020       270     510\n4    NC 2018          6.1       1.8        350    NC 2018       200     610\n5    NC 2019          5.9       1.6        375    NC 2019       210     590\n6    NC 2020          5.3       1.8        400    NC 2020       220     530\n\n\n\n\nInequality joins\nConfession I have never really understood how inequality joins work in regular dplyr but I am sure at some point I am going to need them and now when the stakes are so low is a good time to do it. So lets just take the data from the dplyr 1.1.0 announcement to do this since we know what the output should be.\n\ncompanies &lt;- tibble(\n  id = c(\"A\", \"B\", \"B\"),\n  since = c(1973, 2009, 2022),\n  name = c(\"Patagonia\", \"RStudio\", \"Posit\")\n)\n\ntransactions &lt;- tibble(\n  company = c(\"A\", \"A\", \"B\", \"B\"),\n  year = c(2019, 2020, 2021, 2023),\n  revenue = c(50, 4, 10, 12)\n)\n\ncompanies = copy_to(con3, companies, overwrite = TRUE)\n\ntransactions = copy_to(con4, transactions, overwrite = TRUE)\n\ndb_con = con3$con\n\nSo the main idea of an inequality join is that we can join by a key in this case company but only keep records from a certain date. The blog post kind of equates it with a filter/WHERE that happens during the join phase. So we would see something like this\n\ntransactions |&gt;\n  inner_join(companies, join_by(company == id, year &gt;= since)) \n\n# Source:   SQL [?? x 5]\n# Database: sqlite 3.51.0 [:memory:]\n  company  year revenue since name     \n  &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    \n1 A        2019      50  1973 Patagonia\n2 A        2020       4  1973 Patagonia\n3 B        2021      10  2009 RStudio  \n4 B        2023      12  2009 RStudio  \n5 B        2023      12  2022 Posit    \n\n\nInstead of two equality statements we would effectively use the same syntax just swapping out the = with &gt;=\n\nquery = \"\n      SELECT * FROM transactions\n      INNER JOIN companies \n      ON(\n      transactions.company = companies.id AND\n      transactions.year &gt;= companies.since\n      )\n\n\"\n\ndbGetQuery(db_con, sql(query))\n\n  company year revenue id since      name\n1       A 2019      50  A  1973 Patagonia\n2       A 2020       4  A  1973 Patagonia\n3       B 2021      10  B  2009   RStudio\n4       B 2023      12  B  2009   RStudio\n5       B 2023      12  B  2022     Posit"
  },
  {
    "objectID": "blog/2024/translating-tidyverse-to-sql/index.html#pivots",
    "href": "blog/2024/translating-tidyverse-to-sql/index.html#pivots",
    "title": "Tidyverse to SQL",
    "section": "Pivots",
    "text": "Pivots\nIn tidyverse parlance we use pivots to change the “shape of the data.” If you are unfamiliar with this idea consider the religion and income data below. You will notice that we have a column for each income bracket or what is sometimes called “wide” data. This may be useful for some question but generally if we want to plot things or do things it will be easier if they are “long” data.\n\n\nCode\ncon5 = src_memdb()\n\nrelig = copy_to(con5, relig_income, overwrite = TRUE)\n\nhead(relig_income, n = 2)\n\n\n# A tibble: 2 × 11\n  religion `&lt;$10k` `$10-20k` `$20-30k` `$30-40k` `$40-50k` `$50-75k` `$75-100k`\n  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 Agnostic      27        34        60        81        76       137        122\n2 Atheist       12        27        37        52        35        70         73\n# ℹ 3 more variables: `$100-150k` &lt;dbl&gt;, `&gt;150k` &lt;dbl&gt;,\n#   `Don't know/refused` &lt;dbl&gt;\n\n\nTo make our data “long” we use pivot_longer and to make data “wide” we use pivot_wider each has their own quirks but the general idea is that we have to tell these functions where to put the old names/where to get the new names and where to put the old values/where to get the new values. So if we wanted to make our data longer we would do something like this.\n\nlong = relig_income |&gt;\n    pivot_longer(-religion,\n                names_to = 'income_bracket',\n                values_to = 'income')\n\nhead(long, n = 2)\n\n# A tibble: 2 × 3\n  religion income_bracket income\n  &lt;chr&gt;    &lt;chr&gt;           &lt;dbl&gt;\n1 Agnostic &lt;$10k              27\n2 Agnostic $10-20k            34\n\n\nIf we wanted to make this wide again all we would do is reverse this with pivot_wider\n\nwide = long |&gt;\n    pivot_wider(names_from = income_bracket, values_from = income)\n\n\n\nThere are ton of additional functionality that will not be covered like dealing with not uniquely identified columns.\nTo get a sense of how to do this let’s consult our old friend show_query\n\nrelig |&gt;\n    pivot_longer(-religion,\n                names_to = 'income_bracket',\n                values_to = 'income') |&gt;\n                    show_query()\n\nWe are not going to actually show the results because it is quite the query. The summary of what is happening is that SQLite doesn’t have a perfect equivalent of pivot_longer. Basically, what you need to do is to keep appending smaller and smaller data frames to each other until you get to a long data frame. In other flavors of SQL this process is a lot more humane with explicit PIVOT and UNPIVOT but I am not in one of those flavors. To spare myself a bit I am just going to do two columns\n\ntbl(con5, sql(\"\n    SELECT religion, '&lt;$10k' AS income_bracket, '&lt;$10k' AS income\n    FROM relig_income \n\n    UNION ALL\n\n    SELECT religion, '$10-20k' AS income_bracket, '$10-20k' AS income\n    FROM relig_income\n\n    UNION ALL\n\n    SELECT religion, '$20-30k' AS income_bracket, '$20-30k' AS income\n    FROM relig_income\n\n    \n\"))\n\n# Source:   SQL [?? x 3]\n# Database: sqlite 3.51.0 [:memory:]\n   religion                income_bracket income\n   &lt;chr&gt;                   &lt;chr&gt;          &lt;chr&gt; \n 1 Agnostic                &lt;$10k          &lt;$10k \n 2 Atheist                 &lt;$10k          &lt;$10k \n 3 Buddhist                &lt;$10k          &lt;$10k \n 4 Catholic                &lt;$10k          &lt;$10k \n 5 Don’t know/refused      &lt;$10k          &lt;$10k \n 6 Evangelical Prot        &lt;$10k          &lt;$10k \n 7 Hindu                   &lt;$10k          &lt;$10k \n 8 Historically Black Prot &lt;$10k          &lt;$10k \n 9 Jehovah's Witness       &lt;$10k          &lt;$10k \n10 Jewish                  &lt;$10k          &lt;$10k \n# ℹ more rows\n\n\nI am a little scared to see what this looks for pivot_wider but we should at least give it a go.\n\nlong = relig |&gt;\n    pivot_longer(-religion,\n                 names_to = 'income_bracket',\n                 values_to = 'income')\n\nlong |&gt;\n    pivot_wider(names_from = income_bracket, values_from = income) |&gt;\n    show_query()\n\nOkay again this is a little unwieldy to show. Basically what happens is that we are creating a big case_when condition and then from there we are going to use the same binding trick and then group the data. So lets go ahead and copy and paste some of this.\n\n\nCode\nquery = \"\nSELECT\n    religion,\n    MAX(CASE WHEN (income_bracket = '&lt;$10k') THEN income END) AS '&lt;$10K',\n    MAX(CASE WHEN (income_bracket = '$10-20k') THEN income END) AS '$10-20k',\n    MAX(CASE WHEN (income_bracket = '$20-30k') THEN income END) AS '$20-30k'\nFROM (\n    SELECT religion, '&lt;$10k' AS income_bracket, '&lt;$10k' AS income\n    FROM relig_income\n\n    UNION ALL\n\n    SELECT religion, '$10-20k' AS income_bracket, '$10-20k' AS income\n    FROM relig_income\n\n    UNION ALL\n\n    SELECT religion, '$20-30k' AS income_bracket, '$20-30k' AS income\n    FROM relig_income\n) AS wide_religion\nGROUP BY religion\n\"\n\ntbl(con5, sql(query))\n\n\n# Source:   SQL [?? x 4]\n# Database: sqlite 3.51.0 [:memory:]\n   religion                `&lt;$10K` `$10-20k` `$20-30k`\n   &lt;chr&gt;                   &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;    \n 1 Agnostic                &lt;$10k   $10-20k   $20-30k  \n 2 Atheist                 &lt;$10k   $10-20k   $20-30k  \n 3 Buddhist                &lt;$10k   $10-20k   $20-30k  \n 4 Catholic                &lt;$10k   $10-20k   $20-30k  \n 5 Don’t know/refused      &lt;$10k   $10-20k   $20-30k  \n 6 Evangelical Prot        &lt;$10k   $10-20k   $20-30k  \n 7 Hindu                   &lt;$10k   $10-20k   $20-30k  \n 8 Historically Black Prot &lt;$10k   $10-20k   $20-30k  \n 9 Jehovah's Witness       &lt;$10k   $10-20k   $20-30k  \n10 Jewish                  &lt;$10k   $10-20k   $20-30k  \n11 Mainline Prot           &lt;$10k   $10-20k   $20-30k  \n12 Mormon                  &lt;$10k   $10-20k   $20-30k  \n13 Muslim                  &lt;$10k   $10-20k   $20-30k  \n14 Orthodox                &lt;$10k   $10-20k   $20-30k  \n15 Other Christian         &lt;$10k   $10-20k   $20-30k  \n16 Other Faiths            &lt;$10k   $10-20k   $20-30k  \n17 Other World Religions   &lt;$10k   $10-20k   $20-30k  \n18 Unaffiliated            &lt;$10k   $10-20k   $20-30k"
  },
  {
    "objectID": "blog/2024/translating-tidyverse-to-sql/index.html#unnesta-brief-aside",
    "href": "blog/2024/translating-tidyverse-to-sql/index.html#unnesta-brief-aside",
    "title": "Tidyverse to SQL",
    "section": "Unnest/a brief aside",
    "text": "Unnest/a brief aside\nSo one thing that you come across from time to time in R and python data wrangling are list columns. These happen for a variety of reasons and are pretty innocuous to handle.\n\nlist_starwars = starwars |&gt;\n    select(name, films)\n\n list_starwars |&gt;\n    unnest_longer(films)\n\n# A tibble: 173 × 2\n   name           films                  \n   &lt;chr&gt;          &lt;chr&gt;                  \n 1 Luke Skywalker A New Hope             \n 2 Luke Skywalker The Empire Strikes Back\n 3 Luke Skywalker Return of the Jedi     \n 4 Luke Skywalker Revenge of the Sith    \n 5 Luke Skywalker The Force Awakens      \n 6 C-3PO          A New Hope             \n 7 C-3PO          The Empire Strikes Back\n 8 C-3PO          Return of the Jedi     \n 9 C-3PO          The Phantom Menace     \n10 C-3PO          Attack of the Clones   \n# ℹ 163 more rows\n\n\nHowever, per this Stack overflow answer and the linked question this is not really a thing or like really not advised. Even when you try to copy the starwars dataset to a database you get an error."
  },
  {
    "objectID": "blog/2024/translating-tidyverse-to-sql/index.html#misc",
    "href": "blog/2024/translating-tidyverse-to-sql/index.html#misc",
    "title": "Tidyverse to SQL",
    "section": "Misc",
    "text": "Misc"
  },
  {
    "objectID": "blog/2024/translating-tidyverse-to-sql/index.html#ranking",
    "href": "blog/2024/translating-tidyverse-to-sql/index.html#ranking",
    "title": "Tidyverse to SQL",
    "section": "Ranking",
    "text": "Ranking\nThere are lots of different ways to rank things in R if we want to return the min/max you can do\n\npengs |&gt;\n    slice_max(bill_length_mm, n = 3)\n\n# Source:   SQL [?? x 8]\n# Database: sqlite 3.51.0 [:memory:]\n  species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;     &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Gentoo    Biscoe           59.6          17                 230        6050\n2 Chinstrap Dream            58            17.8               181        3700\n3 Gentoo    Biscoe           55.9          17                 228        5600\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;int&gt;\n\n\nThere are also various ranking functions.\n\nexample = tribble(~id, ~col1,\n                   1, 1,\n                   2, 2,\n                   3, 2,\n                   4, 3,\n                   5, 4)\n\nexample |&gt;\n    mutate(rank_one = dense_rank(col1),\n           rank_two = min_rank(col1))\n\n# A tibble: 5 × 4\n     id  col1 rank_one rank_two\n  &lt;dbl&gt; &lt;dbl&gt;    &lt;int&gt;    &lt;int&gt;\n1     1     1        1        1\n2     2     2        2        2\n3     3     2        2        2\n4     4     3        3        4\n5     5     4        4        5\n\n\nLike our dplyr join functions the dense_rank and min_rank function actually takes inspiration from SQL. So in our example where the two functions differ is how they handle ties. So in dense_rank and min_rank both id 2 and 3 get assigned the same rank where they differ is dense_rank will assign id 4 the rank of 3 and min_rank will assign id 4 the rank of 4.\nSo how would we do this in SQL\n\ncon7 = src_memdb()\n\nteam_rankings = copy_to(con7, example)\n\nteam_rankings |&gt;\n    mutate(rank_one = dense_rank(col1)) |&gt;\n    show_query()\n\n&lt;SQL&gt;\nSELECT\n  `example`.*,\n  CASE\nWHEN (NOT((`col1` IS NULL))) THEN DENSE_RANK() OVER (PARTITION BY (CASE WHEN ((`col1` IS NULL)) THEN 1 ELSE 0 END) ORDER BY `col1`)\nEND AS `rank_one`\nFROM `example`\n\n\nThis is deceptively a bit more complex. So lets break it down.\n\ntbl(con7, sql(\"\nSELECT\nexample .*,\n    CASE \nWHEN (NOT((col1 is NULL))) THEN DENSE_RANK() OVER (PARTITION BY (CASE WHEN ((col1 is NULL)) THEN 1 ELSE 0 END) ORDER BY col1)\nEND AS rank_one\nFROM example\n\"))\n\n# Source:   SQL [?? x 3]\n# Database: sqlite 3.51.0 [:memory:]\n     id  col1 rank_one\n  &lt;dbl&gt; &lt;dbl&gt;    &lt;int&gt;\n1     1     1        1\n2     2     2        2\n3     3     2        2\n4     4     3        3\n5     5     4        4\n\n\nSo basically the PARTITION BY bit is used to divide the data into groups before we rank them. The CASE WHEN handles when we have missing values. Then the window function is applying dense rank over these partions. This was a somewhat silly example so lets do something a bit more realistic. Lets say we actually want to rank the penguins by average bill length and then return the penguins in the top 3.\n\ntbl(con, sql(\n    \"\n    SELECT\n    ranked_pengs .*,\n    CASE\n    WHEN (NOT((avg_bill_length is NULL))) THEN DENSE_RANK() OVER (PARTITION BY (CASE WHEN ((avg_bill_length is NULL)) THEN 1 ELSE 0 END) ORDER BY avg_bill_length)\n    END AS rank\n    FROM( \n     SELECT pengs .*, AVG(bill_length_mm) OVER () AS avg_bill_length\n     FROM pengs)\n     AS ranked_pengs \n     LIMIT 3\n    \"\n))\n\n# Source:   SQL [?? x 10]\n# Database: sqlite 3.51.0 [:memory:]\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n# ℹ 4 more variables: sex &lt;chr&gt;, year &lt;int&gt;, avg_bill_length &lt;dbl&gt;, rank &lt;int&gt;\n\n\nWe could also do this by groups by just inserting a group by statement before the limit bit\n\ntbl(con, sql(\n    \"\n    SELECT\n    ranked_pengs .*,\n    CASE\n    WHEN (NOT((avg_bill_length is NULL))) THEN DENSE_RANK() OVER (PARTITION BY (CASE WHEN ((avg_bill_length is NULL)) THEN 1 ELSE 0 END) ORDER BY avg_bill_length)\n    END AS rank\n    FROM( \n     SELECT pengs .*, AVG(bill_length_mm) OVER () AS avg_bill_length\n     FROM pengs)\n     AS ranked_pengs \n     GROUP BY species\n     LIMIT 3\n    \"\n))\n\n# Source:   SQL [?? x 10]\n# Database: sqlite 3.51.0 [:memory:]\n  species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie    Torgersen           39.1          18.7               181        3750\n2 Chinstrap Dream               46.5          17.9               192        3500\n3 Gentoo    Biscoe              46.1          13.2               211        4500\n# ℹ 4 more variables: sex &lt;chr&gt;, year &lt;int&gt;, avg_bill_length &lt;dbl&gt;, rank &lt;int&gt;"
  },
  {
    "objectID": "blog/2024/translating-tidyverse-to-sql/index.html#distinct-values",
    "href": "blog/2024/translating-tidyverse-to-sql/index.html#distinct-values",
    "title": "Tidyverse to SQL",
    "section": "Distinct Values",
    "text": "Distinct Values\nDuplicates are a fact of life but depending on your question or what information you are trying to show repeated records may not be desirable. We handle these with the same function but kind of like mutate we have to let select handle these. If we wanted one row per column without having to specify every column in our dataset than we could do something like this\n\ntbl(con, sql(\"SELECT *\n            FROM(\n            SELECT pengs .*,\n            ROW_NUMBER() OVER (PARTITION BY species ORDER BY species) AS id \n            FROM PENGS) AS small_pengs\n            WHERE id = 1\"))\n\n# Source:   SQL [?? x 9]\n# Database: sqlite 3.51.0 [:memory:]\n  species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie    Torgersen           39.1          18.7               181        3750\n2 Chinstrap Dream               46.5          17.9               192        3500\n3 Gentoo    Biscoe              46.1          13.2               211        4500\n# ℹ 3 more variables: sex &lt;chr&gt;, year &lt;int&gt;, id &lt;int&gt;\n\n\nHowever if we have a slightly less complex query than we can feed distinct multiple columns\n\ntbl(con, sql(\"SELECT DISTINCT species, island\n            FROM pengs\"))\n\n# Source:   SQL [?? x 2]\n# Database: sqlite 3.51.0 [:memory:]\n  species   island   \n  &lt;chr&gt;     &lt;chr&gt;    \n1 Adelie    Torgersen\n2 Adelie    Biscoe   \n3 Adelie    Dream    \n4 Gentoo    Biscoe   \n5 Chinstrap Dream"
  },
  {
    "objectID": "blog/2024/translating-tidyverse-to-sql/index.html#the-endfor-now",
    "href": "blog/2024/translating-tidyverse-to-sql/index.html#the-endfor-now",
    "title": "Tidyverse to SQL",
    "section": "The End…for now",
    "text": "The End…for now\nI am sure this will end up growing as I think of more than things in R that I need to be able to do in SQL."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "January 1, 2022\n        \n        \n            What to do when you break R\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    zsh\n                \n                \n            \n            \n\n            What happens when you keep getting c stack usage errors \n        \n        \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section",
    "href": "blog/index.html#section",
    "title": "Blog",
    "section": "",
    "text": "January 1, 2022\n        \n        \n            What to do when you break R\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    zsh\n                \n                \n            \n            \n\n            What happens when you keep getting c stack usage errors \n        \n        \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-1",
    "href": "blog/index.html#section-1",
    "title": "Blog",
    "section": "2024",
    "text": "2024\n\n\n    \n    \n                  \n            \n        \n        \n            Tidyverse to SQL\n\n            \n\n            \n        \n        \n    \n    \n    \n                  \n            November 19, 2025\n        \n        \n            R to Python: Just the basics\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    python\n                \n                \n            \n            \n\n            This is me learning the snake language\n        \n        \n    \n    \n    \n                  \n            November 19, 2025\n        \n        \n            Translating What I know in the tidyverse to polars:\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    python\n                \n                \n                \n                    polars\n                \n                \n            \n            \n\n            This is me learning the snake language\n        \n        \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-2",
    "href": "blog/index.html#section-2",
    "title": "Blog",
    "section": "2025",
    "text": "2025\n\n\n    \n    \n                  \n            \n        \n        \n            Modeling Receiver Ability Bayesiansly\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    python\n                \n                \n                \n                    bayes\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    pymc\n                \n                \n                \n                    polars\n                \n                \n            \n            \n\n            How Good is Your Favorite Wide Receiver?\n        \n        \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research and Projects",
    "section": "",
    "text": "Allen Joshua. “The Use of The Holocaust in Online Discourse and in Media: A Computational Approach”\nAllen Joshua. “Collective Memory and Contemporary Political Behavior: Evidence from France”"
  },
  {
    "objectID": "research/index.html#working-papers",
    "href": "research/index.html#working-papers",
    "title": "Research and Projects",
    "section": "",
    "text": "Allen Joshua. “The Use of The Holocaust in Online Discourse and in Media: A Computational Approach”\nAllen Joshua. “Collective Memory and Contemporary Political Behavior: Evidence from France”"
  },
  {
    "objectID": "research/index.html#dormant-papers",
    "href": "research/index.html#dormant-papers",
    "title": "Research and Projects",
    "section": "Dormant Papers",
    "text": "Dormant Papers\n\nAllen Joshua. Testing The Effects of U.S. Airstrikes on Insurgent Initiated Violence in Yemen"
  }
]